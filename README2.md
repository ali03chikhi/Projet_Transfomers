# ğŸ¯ Projet Devoir : Fine-Tuning de Depth Anything avec LoRA

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.30+-FFD21E?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

</div>

---

## ğŸ‘¥ Auteurs

| Nom                  |
| -------------------- | 
| **Abdelali Chikhi** | 
| **Ayman Zejli**      | 
| **Mouad Azenag**      | 
| **Loic Magnan**      | 

---

## ğŸ“– Contexte et Objectif

### Contexte

L'estimation de profondeur monoculaire est une tÃ¢che fondamentale en vision par ordinateur qui consiste Ã  prÃ©dire la distance des objets dans une scÃ¨ne Ã  partir d'une seule image RGB. Cette capacitÃ© est cruciale pour de nombreuses applications :

- ğŸš— **VÃ©hicules autonomes** : Navigation et Ã©vitement d'obstacles
- ğŸ¤– **Robotique** : Manipulation d'objets et navigation
- ğŸ­ **Industrie 4.0** : ContrÃ´le qualitÃ© et inspection automatisÃ©e
- ğŸ® **RÃ©alitÃ© augmentÃ©e** : Placement prÃ©cis d'objets virtuels

### Objectif du Projet

Ce projet vise Ã  **adapter le modÃ¨le Depth Anything** (un modÃ¨le prÃ©-entraÃ®nÃ© de pointe pour l'estimation de profondeur) au **jeu de donnÃ©es Zivid** spÃ©cifique Ã  un contexte industriel, en utilisant la technique de **LoRA (Low-Rank Adaptation)** pour un fine-tuning efficace.

#### Pourquoi LoRA ?

- âœ… RÃ©duction drastique des paramÃ¨tres entraÃ®nables (~1.75% des paramÃ¨tres totaux)
- âœ… PrÃ©servation des connaissances du modÃ¨le prÃ©-entraÃ®nÃ©
- âœ… EntraÃ®nement rapide avec moins de ressources GPU
- âœ… Fusion facile des adaptateurs avec le modÃ¨le de base

---

## ğŸ§  Architecture et Algorithmes

### 1. Le ModÃ¨le PrÃ©-entraÃ®nÃ© : Depth Anything V2

**Depth Anything V2** est un modÃ¨le de fondation de pointe pour l'estimation de profondeur monoculaire. Il s'appuie sur une architecture **DPT (Dense Prediction Transformer)** propulsÃ©e par un encodeur **Vision Transformer (ViT)**. Cette architecture permet de capturer des relations globales dans l'image grÃ¢ce au mÃ©canisme d'attention, surpassant les CNNs classiques sur la prÃ©servation des dÃ©tails fins.

**ImplÃ©mentation via Hugging Face :**
Pour ce projet, nous n'avons pas tÃ©lÃ©chargÃ© manuellement les poids depuis le dÃ©pÃ´t GitHub officiel. Nous avons privilÃ©giÃ© l'intÃ©gration native via la bibliothÃ¨que **Transformers** de Hugging Face.

Le modÃ¨le est chargÃ© dynamiquement depuis le **Hugging Face Hub** (ID : `depth-anything/Depth-Anything-V2-Small-hf`). Cette approche simplifie le pipeline (via `AutoModelForDepthEstimation`), assure la compatibilitÃ© des versions et Ã©vite la gestion complexe de fichiers de poids locaux.

#### Architecture du ModÃ¨le

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Depth Anything Small                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Image (H Ã— W Ã— 3)                                    â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Vision Transformer (ViT) Backbone       â”‚        â”‚
â”‚  â”‚  - Patch Embedding (16 Ã— 16 patches)            â”‚        â”‚
â”‚  â”‚  - Multi-Head Self-Attention (Query, Key, Value)â”‚        â”‚
â”‚  â”‚  - Feed-Forward Networks                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚              DPT Decoder Head                   â”‚        â”‚
â”‚  â”‚  - Feature Reassembly                           â”‚        â”‚
â”‚  â”‚  - Progressive Upsampling                       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Output Depth Map (H Ã— W Ã— 1)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CaractÃ©ristiques ClÃ©s

- **ModÃ¨le utilisÃ©** : `LiheYoung/depth-anything-small-hf`
- **ParamÃ¨tres totaux** : ~25.2 millions
- **RÃ©solution d'entrÃ©e** : 518 Ã— 840 pixels (adaptÃ©e aux images Zivid)
- **Sortie** : Carte de profondeur normalisÃ©e [0, 1]

### 2. L'Algorithme de Fine-Tuning : LoRA (Low-Rank Adaptation)

**LoRA** est une technique de fine-tuning efficace qui permet d'adapter de grands modÃ¨les prÃ©-entraÃ®nÃ©s sans modifier leurs poids originaux.

#### Principe MathÃ©matique

Au lieu de mettre Ã  jour les poids $W$ directement, LoRA dÃ©compose la mise Ã  jour en deux matrices de faible rang :

$$
W_{new} = W_{original} + \Delta W = W_{original} + B \cdot A
$$

OÃ¹ :

- $W_{original} \in \mathbb{R}^{d \times k}$ : Poids gelÃ©s du modÃ¨le original
- $A \in \mathbb{R}^{r \times k}$ : Matrice "down-projection" (compresse)
- $B \in \mathbb{R}^{d \times r}$ : Matrice "up-projection" (dÃ©compresse)
- $r$ : Rang (hyperparamÃ¨tre, $r \ll \min(d, k)$)

#### Configuration LoRA UtilisÃ©e

```python
lora_config = LoraConfig(
    r=16,                                    # Rang de la dÃ©composition
    lora_alpha=32,                           # Facteur d'Ã©chelle (Î±/r)
    target_modules=["query", "key", "value"],# Couches d'attention ciblÃ©es
    lora_dropout=0.05,                       # RÃ©gularisation
    bias="none",                             # Pas d'adaptation des biais
)
```

#### Statistiques d'EntraÃ®nement

```
trainable params: 442,368 || all params: 25,227,457 || trainable%: 1.7535
```

---

## ğŸ’» DÃ©tails de l'ImplÃ©mentation et Pipeline (Code)

### A. Pipeline de DonnÃ©es Custom (Dataset Zivid)

Le jeu de donnÃ©es Zivid est un dataset industriel contenant des paires image RGB / profondeur XYZ capturÃ©es par une camÃ©ra Zivid.

#### Structure des Fichiers

```
DATASET_DEVOIR/
â”œâ”€â”€ images/                    # Images RGB (.png)
â”‚   â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_color.png
â”‚   â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_color.png
â”‚   â””â”€â”€ ...
â””â”€â”€ depth/                     # Cartes de profondeur XYZ (.npy)
    â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_rawDepth.npy
    â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_rawDepth.npy
    â””â”€â”€ ...
```

#### Statistiques du Dataset

| MÃ©trique                    | Valeur              |
| ---------------------------- | ------------------- |
| Nombre total d'Ã©chantillons | 58                  |
| RÃ©solution des images       | 1200 Ã— 1944 pixels |
| Profondeur MIN               | 251.74 mm           |
| Profondeur MAX               | 3907.45 mm          |
| Moyenne des profondeurs      | 1542.16 mm          |
| Ã‰cart-type                  | 295.35 mm           |
| Pixels valides (moyenne)     | 68.5%               |


### B. Configuration du ModÃ¨le et LoRA

```python
from transformers import AutoModelForDepthEstimation, AutoImageProcessor
from peft import get_peft_model, LoraConfig

# 1. Chargement du modÃ¨le prÃ©-entraÃ®nÃ©
MODEL_NAME = "LiheYoung/depth-anything-small-hf"
base_model = AutoModelForDepthEstimation.from_pretrained(MODEL_NAME)
image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)

# 2. Configuration LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query", "key", "value"],
    lora_dropout=0.05,
    bias="none",
)

# 3. Application de LoRA
model_lora = get_peft_model(base_model, lora_config)
```
## â–¶ Comment exÃ©cuter le projet

Placez votre dataset dans la structure suivante :
```text
dataset/
  images/
  depth/
```
Puis ouvrez et lancez le notebook :
```text
jupyter notebook Projet_lora.ipynb
```
Le notebook contient toutes les Ã©tapes du pipeline :
```text
-chargement et prÃ©traitement des donnÃ©es

-crÃ©ation du modÃ¨le LoRA

-entraÃ®nement

-calcul des mÃ©triques

-visualisation (RGB, profondeur GT, profondeur prÃ©dite, carte dâ€™erreur)
```
## ğŸ“Š MÃ©triques

Nous utilisons les mÃ©triques classiques de lâ€™estimation de profondeur :
```text
RMSE

AbsRel

Î´â‚, Î´â‚‚, Î´â‚ƒ
```

Les mÃ©triques sont calculÃ©es sur la profondeur normalisÃ©e (inverse depth) afin de garantir la stabilitÃ© de lâ€™entraÃ®nement. Les prÃ©dictions peuvent ensuite Ãªtre reconverties en profondeur rÃ©elle 
ğ‘
Z (en millimÃ¨tres ou mÃ¨tres) pour la visualisation et lâ€™analyse physique.

## ğŸ–¼ Visualisation

Le notebook gÃ©nÃ¨re automatiquement des figures contenant :
```text
lâ€™image RGB

la profondeur de vÃ©ritÃ© terrain (GT)

la profondeur prÃ©dite

lâ€™erreur absolue
```

## ğŸ§ª RÃ©sultats 

AprÃ¨s 15 Ã©poques dâ€™entraÃ®nement :
| MÃ©trique | Valeur |
| -------- | ------ |
| RMSE     | ~0.42  |
| Î´â‚       | ~0.45  |
| Î´â‚ƒ       | ~0.69  |
MalgrÃ© un dataset trÃ¨s rÃ©duit, le modÃ¨le apprend la gÃ©omÃ©trie des pneus empilÃ©s et reconstruit leur forme de maniÃ¨re cohÃ©rente.

## ğŸ“„ Rapport

Un rapport technique complet  est disponible 

Il contient :
```text
la description du problÃ¨me

le dataset

la mÃ©thode (Depth Anything + LoRA)

les fonctions de perte

les mÃ©triques

les rÃ©sultats quantitatifs et qualitatifs
```
