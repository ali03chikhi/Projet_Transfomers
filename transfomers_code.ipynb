{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef434b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# CELLULE 1 : INSTALLATION DES D√âPENDANCES\n",
    "# ===================================================\n",
    "# Note : Si tout est d√©j√† install√©, vous pouvez passer cette cellule.\n",
    "\n",
    "print(\"‚è≥ Installation des librairies principales...\")\n",
    "# On installe tout en une fois pour gagner du temps\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu122 --user\n",
    "%pip install opencv-python pillow numpy matplotlib tqdm scikit-learn --user\n",
    "%pip install transformers huggingface-hub peft accelerate pytorch-msssim --user\n",
    "\n",
    "print(\"‚úÖ Installation termin√©e. SI C'EST LA PREMI√àRE FOIS, RED√âMARREZ LE KERNEL (Menu Kernel > Restart).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356357e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\chikh\\anaconda4\\envs\\tns\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7758649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Import r√©ussi !\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "print(\"Import r√©ussi !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4a4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pretrained_model(model_name=\"depth-anything/Depth-Anything-V2-Small-hf\"):\n",
    "    \"\"\"\n",
    "    Charge le mod√®le Depth Anything V2 pr√©-entra√Æn√© et son processeur d'images\n",
    "    \"\"\"\n",
    "    print(f\" Chargement du mod√®le: {model_name}\")\n",
    "\n",
    "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
    "    base_model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
    "\n",
    "    print(f\"Mod√®le charg√© avec succ√®s\")\n",
    "    return base_model, image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cb68e621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chargement du mod√®le: depth-anything/Depth-Anything-V2-Small-hf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mod√®le charg√© avec succ√®s\n"
     ]
    }
   ],
   "source": [
    "base_model, image_processor = load_pretrained_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5425486f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_model = base_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b757be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 24,785,089\n",
      "Trainable parameters: 24,785,089\n"
     ]
    }
   ],
   "source": [
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71baebc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspection de l'architecture du mod√®le...\n",
      "============================================================\n",
      "STRUCTURE PRINCIPALE :\n",
      "  : DepthAnythingForDepthEstimation\n",
      "  backbone: Dinov2Backbone\n",
      "  backbone.embeddings: Dinov2Embeddings\n",
      "  backbone.embeddings.patch_embeddings: Dinov2PatchEmbeddings\n",
      "  backbone.embeddings.dropout: Dropout\n",
      "  backbone.encoder: Dinov2Encoder\n",
      "  backbone.encoder.layer: ModuleList\n",
      "  backbone.layernorm: LayerNorm\n",
      "  neck: DepthAnythingNeck\n",
      "  neck.reassemble_stage: DepthAnythingReassembleStage\n",
      "  neck.reassemble_stage.layers: ModuleList\n",
      "  neck.convs: ModuleList\n",
      "  neck.convs.0: Conv2d\n",
      "  neck.convs.1: Conv2d\n",
      "  neck.convs.2: Conv2d\n",
      "  neck.convs.3: Conv2d\n",
      "  neck.fusion_stage: DepthAnythingFeatureFusionStage\n",
      "  neck.fusion_stage.layers: ModuleList\n",
      "  head: DepthAnythingDepthEstimationHead\n",
      "  head.conv1: Conv2d\n",
      "  head.conv2: Conv2d\n",
      "  head.activation1: ReLU\n",
      "  head.conv3: Conv2d\n",
      "  head.activation2: ReLU\n",
      "\n",
      "============================================================\n",
      "COUCHES CIBLES POUR LORA (Attention & Dense) :\n",
      "  backbone.encoder.layer.0.attention.attention.query.weight: torch.Size([384, 384])\n",
      "  backbone.encoder.layer.0.attention.attention.query.bias: torch.Size([384])\n",
      "  backbone.encoder.layer.0.attention.attention.value.weight: torch.Size([384, 384])\n",
      "  backbone.encoder.layer.0.attention.attention.value.bias: torch.Size([384])\n",
      "  backbone.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
      "\n",
      "... et 67 autres couches similaires.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "#NSPECTION DE L'ARCHITECTURE (VERSION NETTE)\n",
    "# ===================================================\n",
    "print(\"Inspection de l'architecture du mod√®le...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# On affiche juste les grands blocs pour comprendre la structure\n",
    "print(\"STRUCTURE PRINCIPALE :\")\n",
    "seen_types = set()\n",
    "for name, module in base_model.named_modules():\n",
    "    # On affiche seulement le premier niveau de profondeur ou les blocs importants\n",
    "    if name.count('.') <= 2: \n",
    "        print(f\"  {name}: {module.__class__.__name__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COUCHES CIBLES POUR LORA (Attention & Dense) :\")\n",
    "# On cherche les couches qu'on va modifier\n",
    "count = 0\n",
    "for name, param in base_model.named_parameters():\n",
    "    if \"query\" in name or \"value\" in name or \"dense\" in name:\n",
    "        if count < 5: # On affiche juste les 5 premi√®res pour l'exemple\n",
    "            print(f\"  {name}: {param.shape}\")\n",
    "        count += 1\n",
    "\n",
    "print(f\"\\n... et {count - 5} autres couches similaires.\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a815a",
   "metadata": {},
   "source": [
    "Preparation de donn√©es "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e5cbbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path_images = r\"C:\\Users\\chikh\\Downloads\\DATASET_DEVOIR (1)\\DATASET_DEVOIR\\images\"\n",
    "path_depths = r\"C:\\Users\\chikh\\Downloads\\DATASET_DEVOIR (1)\\DATASET_DEVOIR\\depth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d0b0b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Dossier Images vis√© : C:\\Users\\chikh\\Downloads\\DATASET_DEVOIR (1)\\DATASET_DEVOIR\\images\n",
      "üìÇ Dossier Depth vis√©  : C:\\Users\\chikh\\Downloads\\DATASET_DEVOIR (1)\\DATASET_DEVOIR\\depth\n",
      "\n",
      "‚úÖ Les dossiers existent !\n",
      "   -> Trouv√© 58 images\n",
      "   -> Trouv√© 58 fichiers XYZ (.npy)\n"
     ]
    }
   ],
   "source": [
    "print(f\"üìÇ Dossier Images vis√© : {path_images}\")\n",
    "print(f\"üìÇ Dossier Depth vis√©  : {path_depths}\")\n",
    "\n",
    "# V√©rification que les dossiers existent\n",
    "if os.path.exists(path_images) and os.path.exists(path_depths):\n",
    "    print(\"\\n‚úÖ Les dossiers existent !\")\n",
    "    \n",
    "    # Compter les fichiers pour √™tre s√ªr\n",
    "    nb_img = len([f for f in os.listdir(path_images) if f.endswith(('.png', '.jpg'))])\n",
    "    nb_npy = len([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
    "    \n",
    "    print(f\"   -> Trouv√© {nb_img} images\")\n",
    "    print(f\"   -> Trouv√© {nb_npy} fichiers XYZ (.npy)\")\n",
    "else:\n",
    "    print(\"\\n‚ùå ERREUR : Un des chemins est introuvable.\")\n",
    "    print(\"   V√©rifie l'orthographe ou copie le chemin absolu.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3006697d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D√©marrage de la v√©rification...\n",
      "Bilan:\n",
      "   - Images (.png/.jpg) : 58\n",
      "   - Depths (.npy)      : 58\n"
     ]
    }
   ],
   "source": [
    "# 2. COMPTAGE ET LISTING\n",
    "# ==========================================\n",
    "print(\"D√©marrage de la v√©rification...\")\n",
    "\n",
    "# On liste et on trie pour garantir l'ordre\n",
    "# On ne garde que les fichiers (pas les dossiers cach√©s)\n",
    "files_img = sorted([f for f in os.listdir(path_images) if f.endswith(('.png', '.jpg'))])\n",
    "files_npy = sorted([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
    "\n",
    "num_img = len(files_img)\n",
    "num_npy = len(files_npy)\n",
    "\n",
    "print(f\"Bilan:\")\n",
    "print(f\"   - Images (.png/.jpg) : {num_img}\")\n",
    "print(f\"   - Depths (.npy)      : {num_npy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e47f8e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyse de 58 fichiers .npy pixel par pixel...\n",
      "‚ö†Ô∏è 21-12-03-18-50-31_Zivid_acquisition_rawDepth.npy : 2116980 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-50-34_Zivid_acquisition_rawDepth.npy : 1730646 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-50-37_Zivid_acquisition_rawDepth.npy : 1655943 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-50-39_Zivid_acquisition_rawDepth.npy : 2033433 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-50-55_Zivid_acquisition_rawDepth.npy : 2366760 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-27_Zivid_acquisition_rawDepth.npy : 1982451 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-30_Zivid_acquisition_rawDepth.npy : 1700013 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-32_Zivid_acquisition_rawDepth.npy : 2108175 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-35_Zivid_acquisition_rawDepth.npy : 1727091 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-37_Zivid_acquisition_rawDepth.npy : 1658802 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-40_Zivid_acquisition_rawDepth.npy : 2079984 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-52-52_Zivid_acquisition_rawDepth.npy : 2363454 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-53-33_Zivid_acquisition_rawDepth.npy : 2206116 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-56-54_Zivid_acquisition_rawDepth.npy : 1913988 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-56-57_Zivid_acquisition_rawDepth.npy : 1740375 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-56-59_Zivid_acquisition_rawDepth.npy : 2113485 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-57-02_Zivid_acquisition_rawDepth.npy : 1722759 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-57-05_Zivid_acquisition_rawDepth.npy : 1656819 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-57-07_Zivid_acquisition_rawDepth.npy : 2010390 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-18-57-22_Zivid_acquisition_rawDepth.npy : 2423166 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-34_Zivid_acquisition_rawDepth.npy : 1932471 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-37_Zivid_acquisition_rawDepth.npy : 1716234 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-39_Zivid_acquisition_rawDepth.npy : 2109381 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-42_Zivid_acquisition_rawDepth.npy : 1727142 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-44_Zivid_acquisition_rawDepth.npy : 1657188 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-46_Zivid_acquisition_rawDepth.npy : 1957890 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-05-57_Zivid_acquisition_rawDepth.npy : 2461917 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-06-42_Zivid_acquisition_rawDepth.npy : 2173545 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-06-44_Zivid_acquisition_rawDepth.npy : 2026005 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 21-12-03-19-07-26_Zivid_acquisition_rawDepth.npy : 2314200 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-09-20-11-43-18_Zivid_acquisition_rawDepth.npy : 4496910 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-09-21-11-58-42_Zivid_acquisition_rawDepth.npy : 2465622 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-09-21-14-12-11_Zivid_acquisition_rawDepth.npy : 5428344 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-09-21-14-12-13_Zivid_acquisition_rawDepth.npy : 4657620 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-09-22-14-37-04_Zivid_acquisition_rawDepth.npy : 2971491 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-06-15-18-27_Zivid_acquisition_rawDepth.npy : 2031861 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-07-14-44-32_Zivid_acquisition_rawDepth.npy : 1790343 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-24-13-36-59_Zivid_acquisition_rawDepth.npy : 1835742 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-24-13-39-21_Zivid_acquisition_rawDepth.npy : 2046903 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-25-14-05-12_Zivid_acquisition_rawDepth.npy : 2017629 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-25-14-28-18_Zivid_acquisition_rawDepth.npy : 2112576 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-25-14-38-17_Zivid_acquisition_rawDepth.npy : 1998558 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-25-15-24-26_Zivid_acquisition_rawDepth.npy : 1918743 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-25-16-37-38_Zivid_acquisition_rawDepth.npy : 1997175 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-10-27-15-59-07_Zivid_acquisition_rawDepth.npy : 2412930 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-08-11-04-21_Zivid_acquisition_rawDepth.npy : 2101698 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-08-11-10-08_Zivid_acquisition_rawDepth.npy : 3112302 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-08-11-10-10_Zivid_acquisition_rawDepth.npy : 2252748 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-09-16-34-29_Zivid_acquisition_rawDepth.npy : 1898847 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-09-16-34-31_Zivid_acquisition_rawDepth.npy : 1947366 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-10-15-36-54_Zivid_acquisition_rawDepth.npy : 2060148 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-10-15-39-33_Zivid_acquisition_rawDepth.npy : 2472324 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-10-15-44-07_Zivid_acquisition_rawDepth.npy : 2042913 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-10-15-56-16_Zivid_acquisition_rawDepth.npy : 1881237 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-14-11-10-50_Zivid_acquisition_rawDepth.npy : 1997238 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-15-09-52-08_Zivid_acquisition_rawDepth.npy : 1674318 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-15-16-34-29_Zivid_acquisition_rawDepth.npy : 2775315 valeurs manquantes d√©tect√©es\n",
      "‚ö†Ô∏è 22-11-15-16-37-42_Zivid_acquisition_rawDepth.npy : 2062629 valeurs manquantes d√©tect√©es\n",
      "\n",
      "==================================================\n",
      "üìä BILAN D√âTAILL√â PAR IMAGE\n",
      "==================================================\n",
      "üñºÔ∏è  22-09-21-14-12-11_Zivid_acquisition_rawDepth.npy | Total manquants: 5428344  (NaN:5428344, 0:0)\n",
      "üñºÔ∏è  22-09-21-14-12-13_Zivid_acquisition_rawDepth.npy | Total manquants: 4657620  (NaN:4657620, 0:0)\n",
      "üñºÔ∏è  22-09-20-11-43-18_Zivid_acquisition_rawDepth.npy | Total manquants: 4496910  (NaN:4496910, 0:0)\n",
      "üñºÔ∏è  22-11-08-11-10-08_Zivid_acquisition_rawDepth.npy | Total manquants: 3112302  (NaN:3112302, 0:0)\n",
      "üñºÔ∏è  22-09-22-14-37-04_Zivid_acquisition_rawDepth.npy | Total manquants: 2971491  (NaN:2971491, 0:0)\n",
      "üñºÔ∏è  22-11-15-16-34-29_Zivid_acquisition_rawDepth.npy | Total manquants: 2775315  (NaN:2775315, 0:0)\n",
      "üñºÔ∏è  22-11-10-15-39-33_Zivid_acquisition_rawDepth.npy | Total manquants: 2472324  (NaN:2472324, 0:0)\n",
      "üñºÔ∏è  22-09-21-11-58-42_Zivid_acquisition_rawDepth.npy | Total manquants: 2465622  (NaN:2465622, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-57_Zivid_acquisition_rawDepth.npy | Total manquants: 2461917  (NaN:2461917, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-57-22_Zivid_acquisition_rawDepth.npy | Total manquants: 2423166  (NaN:2423166, 0:0)\n",
      "üñºÔ∏è  22-10-27-15-59-07_Zivid_acquisition_rawDepth.npy | Total manquants: 2412930  (NaN:2412930, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-50-55_Zivid_acquisition_rawDepth.npy | Total manquants: 2366760  (NaN:2366760, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-52_Zivid_acquisition_rawDepth.npy | Total manquants: 2363454  (NaN:2363454, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-07-26_Zivid_acquisition_rawDepth.npy | Total manquants: 2314200  (NaN:2314200, 0:0)\n",
      "üñºÔ∏è  22-11-08-11-10-10_Zivid_acquisition_rawDepth.npy | Total manquants: 2252748  (NaN:2252748, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-53-33_Zivid_acquisition_rawDepth.npy | Total manquants: 2206116  (NaN:2206116, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-06-42_Zivid_acquisition_rawDepth.npy | Total manquants: 2173545  (NaN:2173545, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-50-31_Zivid_acquisition_rawDepth.npy | Total manquants: 2116980  (NaN:2116980, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-56-59_Zivid_acquisition_rawDepth.npy | Total manquants: 2113485  (NaN:2113485, 0:0)\n",
      "üñºÔ∏è  22-10-25-14-28-18_Zivid_acquisition_rawDepth.npy | Total manquants: 2112576  (NaN:2112576, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-39_Zivid_acquisition_rawDepth.npy | Total manquants: 2109381  (NaN:2109381, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-32_Zivid_acquisition_rawDepth.npy | Total manquants: 2108175  (NaN:2108175, 0:0)\n",
      "üñºÔ∏è  22-11-08-11-04-21_Zivid_acquisition_rawDepth.npy | Total manquants: 2101698  (NaN:2101698, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-40_Zivid_acquisition_rawDepth.npy | Total manquants: 2079984  (NaN:2079984, 0:0)\n",
      "üñºÔ∏è  22-11-15-16-37-42_Zivid_acquisition_rawDepth.npy | Total manquants: 2062629  (NaN:2062629, 0:0)\n",
      "üñºÔ∏è  22-11-10-15-36-54_Zivid_acquisition_rawDepth.npy | Total manquants: 2060148  (NaN:2060148, 0:0)\n",
      "üñºÔ∏è  22-10-24-13-39-21_Zivid_acquisition_rawDepth.npy | Total manquants: 2046903  (NaN:2046903, 0:0)\n",
      "üñºÔ∏è  22-11-10-15-44-07_Zivid_acquisition_rawDepth.npy | Total manquants: 2042913  (NaN:2042913, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-50-39_Zivid_acquisition_rawDepth.npy | Total manquants: 2033433  (NaN:2033433, 0:0)\n",
      "üñºÔ∏è  22-10-06-15-18-27_Zivid_acquisition_rawDepth.npy | Total manquants: 2031861  (NaN:2031861, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-06-44_Zivid_acquisition_rawDepth.npy | Total manquants: 2026005  (NaN:2026005, 0:0)\n",
      "üñºÔ∏è  22-10-25-14-05-12_Zivid_acquisition_rawDepth.npy | Total manquants: 2017629  (NaN:2017629, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-57-07_Zivid_acquisition_rawDepth.npy | Total manquants: 2010390  (NaN:2010390, 0:0)\n",
      "üñºÔ∏è  22-10-25-14-38-17_Zivid_acquisition_rawDepth.npy | Total manquants: 1998558  (NaN:1998558, 0:0)\n",
      "üñºÔ∏è  22-11-14-11-10-50_Zivid_acquisition_rawDepth.npy | Total manquants: 1997238  (NaN:1997238, 0:0)\n",
      "üñºÔ∏è  22-10-25-16-37-38_Zivid_acquisition_rawDepth.npy | Total manquants: 1997175  (NaN:1997175, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-27_Zivid_acquisition_rawDepth.npy | Total manquants: 1982451  (NaN:1982451, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-46_Zivid_acquisition_rawDepth.npy | Total manquants: 1957890  (NaN:1957890, 0:0)\n",
      "üñºÔ∏è  22-11-09-16-34-31_Zivid_acquisition_rawDepth.npy | Total manquants: 1947366  (NaN:1947366, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-34_Zivid_acquisition_rawDepth.npy | Total manquants: 1932471  (NaN:1932471, 0:0)\n",
      "üñºÔ∏è  22-10-25-15-24-26_Zivid_acquisition_rawDepth.npy | Total manquants: 1918743  (NaN:1918743, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-56-54_Zivid_acquisition_rawDepth.npy | Total manquants: 1913988  (NaN:1913988, 0:0)\n",
      "üñºÔ∏è  22-11-09-16-34-29_Zivid_acquisition_rawDepth.npy | Total manquants: 1898847  (NaN:1898847, 0:0)\n",
      "üñºÔ∏è  22-11-10-15-56-16_Zivid_acquisition_rawDepth.npy | Total manquants: 1881237  (NaN:1881237, 0:0)\n",
      "üñºÔ∏è  22-10-24-13-36-59_Zivid_acquisition_rawDepth.npy | Total manquants: 1835742  (NaN:1835742, 0:0)\n",
      "üñºÔ∏è  22-10-07-14-44-32_Zivid_acquisition_rawDepth.npy | Total manquants: 1790343  (NaN:1790343, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-56-57_Zivid_acquisition_rawDepth.npy | Total manquants: 1740375  (NaN:1740375, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-50-34_Zivid_acquisition_rawDepth.npy | Total manquants: 1730646  (NaN:1730646, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-42_Zivid_acquisition_rawDepth.npy | Total manquants: 1727142  (NaN:1727142, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-35_Zivid_acquisition_rawDepth.npy | Total manquants: 1727091  (NaN:1727091, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-57-02_Zivid_acquisition_rawDepth.npy | Total manquants: 1722759  (NaN:1722759, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-37_Zivid_acquisition_rawDepth.npy | Total manquants: 1716234  (NaN:1716234, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-30_Zivid_acquisition_rawDepth.npy | Total manquants: 1700013  (NaN:1700013, 0:0)\n",
      "üñºÔ∏è  22-11-15-09-52-08_Zivid_acquisition_rawDepth.npy | Total manquants: 1674318  (NaN:1674318, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-52-37_Zivid_acquisition_rawDepth.npy | Total manquants: 1658802  (NaN:1658802, 0:0)\n",
      "üñºÔ∏è  21-12-03-19-05-44_Zivid_acquisition_rawDepth.npy | Total manquants: 1657188  (NaN:1657188, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-57-05_Zivid_acquisition_rawDepth.npy | Total manquants: 1656819  (NaN:1656819, 0:0)\n",
      "üñºÔ∏è  21-12-03-18-50-37_Zivid_acquisition_rawDepth.npy | Total manquants: 1655943  (NaN:1655943, 0:0)\n",
      "\n",
      "==================================================\n",
      "üìà STATISTIQUES GLOBALES\n",
      "--------------------------------------------------\n",
      "‚úÖ Total pixels analys√©s : 405,907,200\n",
      "‚ùå Total valeurs manquantes : 127,848,333\n",
      "‚ö†Ô∏è Taux de corruption global : 31.4969%\n",
      "üìÇ Nombre de fichiers affect√©s : 58 / 58\n"
     ]
    }
   ],
   "source": [
    "# Analyse des donn√©es image par image\n",
    "print(f\"Analyse de {len(files_npy)} fichiers .npy pixel par pixel...\")\n",
    "\n",
    "# Initialisation des compteurs globaux\n",
    "total_nan_global = 0\n",
    "total_zero_global = 0\n",
    "total_inf_global = 0\n",
    "total_pixels_analyses = 0\n",
    "\n",
    "rapport_erreurs = {}\n",
    "\n",
    "for filename in files_npy:\n",
    "    full_path = os.path.join(path_depths, filename)\n",
    "    \n",
    "    try:\n",
    "        data = np.load(full_path)\n",
    "        total_pixels_analyses += data.size\n",
    "        \n",
    "        # D√©tection par type\n",
    "        num_nan = np.count_nonzero(np.isnan(data))\n",
    "        num_zero = np.count_nonzero(data == 0)\n",
    "        num_inf = np.count_nonzero(np.isinf(data))\n",
    "        \n",
    "        # Somme des manquants pour ce fichier pr√©cis\n",
    "        total_manquants_file = num_nan + num_zero + num_inf\n",
    "\n",
    "        # Mise √† jour des compteurs globaux\n",
    "        total_nan_global += num_nan\n",
    "        total_zero_global += num_zero\n",
    "        total_inf_global += num_inf\n",
    "\n",
    "        if total_manquants_file > 0:\n",
    "            # On stocke les d√©tails pour le rapport final\n",
    "            rapport_erreurs[filename] = {\n",
    "                \"nan\": num_nan, \n",
    "                \"zero\": num_zero, \n",
    "                \"inf\": num_inf, \n",
    "                \"total\": total_manquants_file,\n",
    "                \"shape\": data.shape\n",
    "            }\n",
    "            # Affichage imm√©diat pour suivre l'avancement\n",
    "            print(f\"‚ö†Ô∏è {filename} : {total_manquants_file} valeurs manquantes d√©tect√©es\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erreur lecture {filename}: {e}\")\n",
    "\n",
    "# --- R√©sum√© final d√©taill√© ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üìä BILAN D√âTAILL√â PAR IMAGE\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if not rapport_erreurs:\n",
    "    print(\"‚ú® F√©licitations ! Aucun pixel manquant sur l'ensemble du dataset.\")\n",
    "else:\n",
    "    # On trie les erreurs pour afficher les fichiers les plus probl√©matiques en premier\n",
    "    fichiers_tries = sorted(rapport_erreurs.items(), key=lambda x: x[1]['total'], reverse=True)\n",
    "    \n",
    "    for filename, stats in fichiers_tries:\n",
    "        print(f\"üñºÔ∏è  {filename :<25} | Total manquants: {stats['total']:<8} (NaN:{stats['nan']}, 0:{stats['zero']})\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üìà STATISTIQUES GLOBALES\")\n",
    "    print(\"-\" * 50)\n",
    "    total_anomalies = total_nan_global + total_zero_global + total_inf_global\n",
    "    print(f\"‚úÖ Total pixels analys√©s : {total_pixels_analyses:,}\")\n",
    "    print(f\"‚ùå Total valeurs manquantes : {total_anomalies:,}\")\n",
    "    \n",
    "    if total_pixels_analyses > 0:\n",
    "        pourcentage = (total_anomalies / total_pixels_analyses) * 100\n",
    "        print(f\"‚ö†Ô∏è Taux de corruption global : {pourcentage:.4f}%\")\n",
    "    print(f\"üìÇ Nombre de fichiers affect√©s : {len(rapport_erreurs)} / {len(files_npy)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8adae0d",
   "metadata": {},
   "source": [
    "### Gestion des valeurs manquantes\n",
    "Pourquoi ne pas utiliser l'Inpainting ou l'Interpolation ?\n",
    "Bien que l'inpainting (rebouchage) soit courant en vision par ordinateur classique, il comporte un risque majeur pour notre projet industriel : inventer de la mati√®re. Sur un pneu, une rayure ou un d√©faut de gomme peut ressembler √† une \"donn√©e manquante\". Si nous rebouchons artificiellement ces zones, le mod√®le apprendra √† lisser les d√©fauts au lieu de les d√©tecter.\n",
    "\n",
    "Notre Strat√©gie : Le Masquage Dynamique (Valid Mask)\n",
    "Plut√¥t que de modifier les donn√©es brutes (ce qui introduirait du bruit), nous avons opt√© pour une approche \"Data-Centric\" rigoureuse int√©gr√©e directement dans la fonction de perte (Loss Function) :\n",
    "\n",
    "Identification : Lors du chargement, nous cr√©ons un masque binaire (valid_mask) o√π 1 = pixel valide et 0 = pixel manquant (NaN, Inf, ou 0 venant du capteur).\n",
    "\n",
    "Entra√Ænement S√©lectif : Dans notre boucle d'entra√Ænement, la perte (Loss) n'est calcul√©e que sur les pixels o√π le masque vaut 1.\n",
    "\n",
    "Cons√©quence : Le mod√®le est libre de pr√©dire ce qu'il \"pense\" √™tre correct dans les zones vides, mais il n'est jamais p√©nalis√© s'il se trompe sur ces zones invisibles.\n",
    "\n",
    "R√©sultat : Cela permet au mod√®le d'apprendre la structure globale du pneu gr√¢ce aux pixels valides, sans √™tre pollu√© par les artefacts du capteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ac05cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le nombre de fichiers est identique.\n",
      "Tous les noms de fichiers semblent correspondre (logique color <-> rawDepth).\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 3. V√âRIFICATION DE L'ALIGNEMENT\n",
    "# ==========================================\n",
    "if num_img != num_npy:\n",
    "    print(f\"ATTENTION : Le nombre de fichiers ne correspond pas ! (Diff√©rence : {abs(num_img - num_npy)})\")\n",
    "else:\n",
    "    print(\"Le nombre de fichiers est identique.\")\n",
    "\n",
    "# V√©rification des correspondances de noms (optionnel mais recommand√©)\n",
    "# On v√©rifie si image_01_color.png a bien son √©quivalent image_01_rawDepth.npy\n",
    "mismatches = []\n",
    "for img_name in files_img:\n",
    "    # On recr√©e le nom attendu du fichier depth (selon la logique Zivid vue dans ton notebook)\n",
    "    # Ex: \"scan_color.png\" -> \"scan_rawDepth.npy\"\n",
    "    expected_depth_name = img_name.replace(\"_color.png\", \"_rawDepth.npy\").replace(\".png\", \".npy\")\n",
    "    \n",
    "    # Si tes fichiers n'ont pas \"_color\", ajuste cette ligne (ex: replace(\".png\", \".npy\"))\n",
    "    \n",
    "    if expected_depth_name not in files_npy:\n",
    "        mismatches.append((img_name, expected_depth_name))\n",
    "\n",
    "if len(mismatches) > 0:\n",
    "    print(f\"ATTENTION : {len(mismatches)} paires ne correspondent pas au niveau du nom !\")\n",
    "    print(f\"   Exemple de manquant : {mismatches[0]}\")\n",
    "else:\n",
    "    print(\"Tous les noms de fichiers semblent correspondre (logique color <-> rawDepth).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ac00b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Test d'extraction du canal Z sur un fichier al√©atoire...\n",
      "   Fichier : 21-12-03-18-56-54_Zivid_acquisition_rawDepth.npy\n",
      "   Shape originale : (1200, 1944, 3)\n",
      "   Extraction Z r√©ussie ! Shape finale : (1200, 1944)\n",
      "   Val Min (avec NaN) : 1034.1241455078125\n",
      "   Val Max (avec NaN) : 2812.926025390625\n",
      "   Nombre de pixels NaN (vides) : 637996 (27.3%)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. TEST D'EXTRACTION DU Z \n",
    "# ==========================================\n",
    "\n",
    "if num_npy > 0:\n",
    "    print(\"\\n Test d'extraction du canal Z sur un fichier al√©atoire...\")\n",
    "    \n",
    "    # Prendre un fichier au hasard\n",
    "    random_npy = random.choice(files_npy)\n",
    "    full_path = os.path.join(path_depths, random_npy)\n",
    "    \n",
    "    try:\n",
    "        # Chargement\n",
    "        data = np.load(full_path) # Shape (H, W, 3) normalement\n",
    "        \n",
    "        print(f\"   Fichier : {random_npy}\")\n",
    "        print(f\"   Shape originale : {data.shape}\")\n",
    "        \n",
    "        if len(data.shape) == 3 and data.shape[2] == 3:\n",
    "            # EXTRACTION DU Z (Canal index 2)\n",
    "            Z_channel = data[:, :, 2]\n",
    "            \n",
    "            # V√©rif statistiques\n",
    "            print(f\"   Extraction Z r√©ussie ! Shape finale : {Z_channel.shape}\")\n",
    "            print(f\"   Val Min (avec NaN) : {np.nanmin(Z_channel)}\")\n",
    "            print(f\"   Val Max (avec NaN) : {np.nanmax(Z_channel)}\")\n",
    "            \n",
    "            # V√©rification des NaN\n",
    "            nan_count = np.isnan(Z_channel).sum()\n",
    "            print(f\"   Nombre de pixels NaN (vides) : {nan_count} ({(nan_count/Z_channel.size)*100:.1f}%)\")\n",
    "        else:\n",
    "            print(\"Erreur : Le fichier .npy n'a pas 3 canaux (XYZ). V√©rifie le format.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcf42ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier charg√© est de forme : (1200, 1944)\n",
      "La valeur de Profondeur (Z) au pixel [600, 972] est :\n",
      "-> **1365.02 mm**\n"
     ]
    }
   ],
   "source": [
    "filename = \"21-12-03-19-05-46_Zivid_acquisition_rawDepth.npy\"\n",
    "full_path = os.path.join(path_depths, filename)\n",
    "\n",
    "# --- EXTRACTION ---\n",
    "# 1. Chargement de la donn√©e (1200, 1944, 3)\n",
    "data_xyz = np.load(full_path)\n",
    "\n",
    "# 2. Extraction de la carte de profondeur Z (1200, 1944)\n",
    "Z_channel = data_xyz[:, :, 2] \n",
    "\n",
    "# 3. Traitement des NaN (pour s'assurer qu'on peut lire)\n",
    "Z_clean = np.nan_to_num(Z_channel, nan=0.0)\n",
    "\n",
    "# --- LECTURE DE LA VALEUR D'UN PIXEL ---\n",
    "pixel_row = 600\n",
    "pixel_col = 972\n",
    "\n",
    "# Acc√©der √† la valeur Z du pixel (600, 972)\n",
    "profondeur_pixel = Z_clean[pixel_row, pixel_col]\n",
    "\n",
    "print(f\"Le fichier charg√© est de forme : {Z_channel.shape}\")\n",
    "print(f\"La valeur de Profondeur (Z) au pixel [{pixel_row}, {pixel_col}] est :\")\n",
    "print(f\"-> **{profondeur_pixel:.2f} mm**\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6193767",
   "metadata": {},
   "source": [
    "### Pourquoi traiter les donn√©es √† la vol√©e (`__getitem__`) ?\n",
    "\n",
    "Nous avons choisi d'extraire le canal Z et de normaliser les donn√©es directement dans la classe `Dataset` (pendant l'entra√Ænement) plut√¥t que de cr√©er une copie \"propre\" du dataset sur le disque. Ce choix technique est motiv√© par trois facteurs :\n",
    "\n",
    "#### 1. √âconomie de Stockage (Zivid = Lourd)\n",
    "Chaque fichier `.npy` original contient 3 canaux (X, Y, Z). Si nous avions pr√©-trait√© tout le dataset pour sauvegarder uniquement le canal Z, nous aurions d√ª dupliquer des milliers de fichiers, doublant l'espace disque n√©cessaire.\n",
    "* **Approche choisie :** Le fichier XYZ est charg√© en RAM, le canal Z est extrait, et le reste est lib√©r√© imm√©diatement. Z√©ro duplication.\n",
    "\n",
    "#### 2. Flexibilit√© de la R√©solution (Approche Data-Centric)\n",
    "C'est l'avantage majeur. En effectuant le redimensionnement dans `__getitem__`, nous pouvons tester diff√©rentes r√©solutions (518x518, puis 756x1260) simplement en changeant un param√®tre de configuration.\n",
    "* Si nous avions fig√© le dataset en amont, il aurait fallu tout recalculer pour chaque test de r√©solution.\n",
    "\n",
    "#### 3. Gestion Dynamique des Masques\n",
    "Au lieu de \"boucher les trous\" d√©finitivement en rempla√ßant les NaN par 0 dans un fichier, nous g√©n√©rons un **masque de validit√©** √† la vol√©e. Cela permet au mod√®le d'ignorer math√©matiquement les zones vides, sans alt√©rer la donn√©e source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd42a42",
   "metadata": {},
   "source": [
    "Configuration de LoRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4223522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. D√âFINITION DE LA CONFIGURATION LoRA ---\n",
    "\n",
    "# Pour les Vision Transformers, nous ciblons les couches d'attention (Query, Key, Value)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                                    # Le rang de la matrice LoRA (ajustement de la capacit√© d'apprentissage)\n",
    "    lora_alpha=32,                           # Facteur de mise √† l'√©chelle (g√©n√©ralement 2 * r)\n",
    "    target_modules=[\"query\", \"key\", \"value\"],# Les couches sp√©cifiques du ViT √† modifier (Attention)\n",
    "    lora_dropout=0.05,                       # Dropout appliqu√© aux couches LoRA\n",
    "    bias=\"none\",                             # Ne pas adapter les biais\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a06a797e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖMod√®le LoRA appliqu√© !\n",
      "trainable params: 442,368 || all params: 25,227,457 || trainable%: 1.7535\n"
     ]
    }
   ],
   "source": [
    "# La fonction get_peft_model g√®le les poids du mod√®le de base et ajoute les adaptateurs LoRA\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "\n",
    "print(\"‚úÖMod√®le LoRA appliqu√© !\")\n",
    "# Afficher le nombre de param√®tres entra√Æn√©s\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111c32a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def depth_data_collator(features):\n",
    "    return {\n",
    "        \"pixel_values\": torch.stack([x[\"pixel_values\"] for x in features]),\n",
    "        \"labels\": torch.stack([x[\"labels\"] for x in features]), # Chang√© depth_gt -> labels\n",
    "        \"valid_mask\": torch.stack([x[\"valid_mask\"] for x in features]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1e49625e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Calcul des statistiques sur TOUS les fichiers depth...\n",
      "   (Cela peut prendre quelques minutes...)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyse des fichiers depth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:11<00:00,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STATISTIQUES GLOBALES DU DATASET\n",
      "============================================================\n",
      " Profondeur MIN (sur tous les fichiers): 251.74 mm\n",
      " Profondeur MAX (sur tous les fichiers): 3907.45 mm\n",
      " Moyenne des moyennes: 1542.16 mm\n",
      " √âcart-type: 295.35 mm\n",
      " Pixels valides (moyenne): 68.5%\n",
      "============================================================\n",
      "\n",
      " Utilisez ces valeurs dans votre Dataset:\n",
      "   depth_min = 251.74\n",
      "   depth_max = 3907.45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# CALCUL DES STATISTIQUES GLOBALES DU DATASET\n",
    "# ===================================================\n",
    "print(\" Calcul des statistiques sur TOUS les fichiers depth...\")\n",
    "print(\"   (Cela peut prendre quelques minutes...)\\n\")\n",
    "\n",
    "depth_stats = {\n",
    "    'min_values': [],\n",
    "    'max_values': [],\n",
    "    'mean_values': [],\n",
    "    'valid_percentages': []\n",
    "}\n",
    "\n",
    "# Parcourir TOUS les fichiers .npy\n",
    "from tqdm import tqdm  # Pour une barre de progression\n",
    "\n",
    "for depth_file in tqdm(files_npy, desc=\"Analyse des fichiers depth\"):\n",
    "    depth_path = os.path.join(path_depths, depth_file)\n",
    "    \n",
    "    # Charger et extraire Z\n",
    "    depth_xyz = np.load(depth_path)\n",
    "    depth_z = depth_xyz[:, :, 2]\n",
    "    \n",
    "    # Calculer statistiques (en ignorant NaN)\n",
    "    if not np.all(np.isnan(depth_z)):  # Si au moins une valeur valide\n",
    "        depth_stats['min_values'].append(np.nanmin(depth_z))\n",
    "        depth_stats['max_values'].append(np.nanmax(depth_z))\n",
    "        depth_stats['mean_values'].append(np.nanmean(depth_z))\n",
    "        \n",
    "        # % de pixels valides\n",
    "        valid_pct = (~np.isnan(depth_z)).sum() / depth_z.size * 100\n",
    "        depth_stats['valid_percentages'].append(valid_pct)\n",
    "\n",
    "# Calculer les statistiques globales\n",
    "GLOBAL_MIN = np.min(depth_stats['min_values'])\n",
    "GLOBAL_MAX = np.max(depth_stats['max_values'])\n",
    "GLOBAL_MEAN = np.mean(depth_stats['mean_values'])\n",
    "GLOBAL_STD = np.std(depth_stats['mean_values'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STATISTIQUES GLOBALES DU DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\" Profondeur MIN (sur tous les fichiers): {GLOBAL_MIN:.2f} mm\")\n",
    "print(f\" Profondeur MAX (sur tous les fichiers): {GLOBAL_MAX:.2f} mm\")\n",
    "print(f\" Moyenne des moyennes: {GLOBAL_MEAN:.2f} mm\")\n",
    "print(f\" √âcart-type: {GLOBAL_STD:.2f} mm\")\n",
    "print(f\" Pixels valides (moyenne): {np.mean(depth_stats['valid_percentages']):.1f}%\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Ces valeurs vont servir pour la normalisation!\n",
    "print(f\"\\n Utilisez ces valeurs dans votre Dataset:\")\n",
    "print(f\"   depth_min = {GLOBAL_MIN:.2f}\")\n",
    "print(f\"   depth_max = {GLOBAL_MAX:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5242967c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total de fichiers: 58\n",
      " Split effectu√©:\n",
      "   Train: 46 √©chantillons\n",
      "   Val:   12 √©chantillons\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# SPLIT TRAIN / VALIDATION\n",
    "# ===================================================\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Vos fichiers d√©j√† tri√©s\n",
    "files_img = sorted([f for f in os.listdir(path_images) if f.endswith('.png')])\n",
    "files_npy = sorted([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
    "\n",
    "print(f\" Total de fichiers: {len(files_img)}\")\n",
    "\n",
    "# Split 80% train, 20% validation\n",
    "train_imgs, val_imgs, train_depths, val_depths = train_test_split(\n",
    "    files_img, files_npy, \n",
    "    test_size=0.2,  # 20% validation\n",
    "    random_state=42,  # Pour reproductibilit√©\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "print(f\" Split effectu√©:\")\n",
    "print(f\"   Train: {len(train_imgs)} √©chantillons\")\n",
    "print(f\"   Val:   {len(val_imgs)} √©chantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95419e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class ZividDepthDataset(Dataset):\n",
    "    def __init__(self, image_folder, depth_folder, image_files, depth_files, processor, depth_min, depth_max, use_inverse=True):\n",
    "        self.image_folder = image_folder\n",
    "        self.depth_folder = depth_folder\n",
    "        self.image_files = image_files\n",
    "        self.depth_files = depth_files\n",
    "        self.processor = processor\n",
    "        self.depth_min = depth_min\n",
    "        self.depth_max = depth_max\n",
    "        self.use_inverse = use_inverse\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        depth_path = os.path.join(self.depth_folder, self.depth_files[idx])\n",
    "        depth_map = np.load(depth_path)\n",
    "        \n",
    "        # Correction Canal Z (Profondeur)\n",
    "        if depth_map.ndim == 3:\n",
    "            depth_map = depth_map[:, :, 2]\n",
    "        \n",
    "        # Masque avant normalisation\n",
    "        valid_mask = (depth_map > 0) & (depth_map < 10000) & (~np.isnan(depth_map))\n",
    "        depth_map = np.nan_to_num(depth_map, nan=0.0)\n",
    "\n",
    "        # Normalisation\n",
    "        if self.use_inverse:\n",
    "            depth_inv = 1.0 / (depth_map + 1e-6)\n",
    "            depth_min_inv = 1.0 / self.depth_max\n",
    "            depth_max_inv = 1.0 / self.depth_min\n",
    "            depth_normalized = (depth_inv - depth_min_inv) / (depth_max_inv - depth_min_inv + 1e-6)\n",
    "        else:\n",
    "            depth_normalized = (depth_map - self.depth_min) / (self.depth_max - self.depth_min + 1e-6)\n",
    "        \n",
    "        depth_normalized = np.clip(depth_normalized, 0.0, 1.0)\n",
    "        \n",
    "        # üî• HAUTE R√âSOLUTION : On impose 756x1260 (multiple de 14)\n",
    "        inputs = self.processor(images=image, return_tensors=\"pt\", size={\"height\": 756, \"width\": 1260})\n",
    "        pixel_values = inputs.pixel_values.squeeze(0)\n",
    "        \n",
    "        return {\n",
    "            \"pixel_values\": pixel_values,\n",
    "            \"labels\": torch.from_numpy(depth_normalized).float(),\n",
    "            \"valid_mask\": torch.from_numpy(valid_mask).float()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79a2ff60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\chikh\\anaconda4\\envs\\tns\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "class DepthTrainerFinal(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        pixel_values = inputs[\"pixel_values\"]\n",
    "        labels = inputs[\"labels\"]\n",
    "        mask = inputs.get(\"valid_mask\", None)\n",
    "        \n",
    "        # Inf√©rence\n",
    "        outputs = model(pixel_values=pixel_values)\n",
    "        pred = outputs.predicted_depth\n",
    "        \n",
    "        # 1. Mise √† l'√©chelle (Bicubic pour pr√©server les courbes des pneus)\n",
    "        # On passe de la r√©solution interne (ex: 756x1260) √† la r√©solution du label (1200x1944)\n",
    "        pred = F.interpolate(\n",
    "            pred.unsqueeze(1), \n",
    "            size=labels.shape[-2:], \n",
    "            mode=\"bicubic\", \n",
    "            align_corners=False\n",
    "        ).squeeze(1)\n",
    "        \n",
    "        # 2. Masquage des zones valides (√©vite d'apprendre sur les NaN/vides)\n",
    "        valid_mask = (mask > 0) & (labels > 0)\n",
    "        \n",
    "        # 3. Perte L1 (Pr√©cision globale de la distance)\n",
    "        l1_loss = F.l1_loss(pred[valid_mask], labels[valid_mask])\n",
    "        \n",
    "        # 4. Perte de Gradient (Nettet√© des contours)\n",
    "        def get_grad(x):\n",
    "            # Calcule la diff√©rence entre pixels adjacents\n",
    "            dx = x[:, :, 1:] - x[:, :, :-1]\n",
    "            dy = x[:, 1:, :] - x[:, :-1, :]\n",
    "            return dx, dy\n",
    "\n",
    "        pdx, pdy = get_grad(pred)\n",
    "        gdx, gdy = get_grad(labels)\n",
    "        \n",
    "        # Masques pour les gradients (les deux pixels voisins doivent √™tre valides)\n",
    "        mask_x = valid_mask[:, :, 1:] * valid_mask[:, :, :-1]\n",
    "        mask_y = valid_mask[:, 1:, :] * valid_mask[:, :-1, :]\n",
    "\n",
    "        # Poids : On multiplie par 10 l'importance des pixels qui sont des bords (fort gradient)\n",
    "        weight_x = 1.0 + 10.0 * (torch.abs(gdx) > 0.02).float() \n",
    "        weight_y = 1.0 + 10.0 * (torch.abs(gdy) > 0.02).float()\n",
    "        \n",
    "        # Somme des erreurs de gradient normalis√©e par le nombre de pixels valides\n",
    "        grad_loss_x = (torch.abs(pdx - gdx) * weight_x * mask_x).sum() / (mask_x.sum() + 1e-6)\n",
    "        grad_loss_y = (torch.abs(pdy - gdy) * weight_y * mask_y).sum() / (mask_y.sum() + 1e-6)\n",
    "        \n",
    "        grad_loss = grad_loss_x + grad_loss_y\n",
    "        \n",
    "        # 5. PERTE TOTALE pond√©r√©e\n",
    "        loss = l1_loss + 3.0 * grad_loss\n",
    "        \n",
    "        # Retour indispensable pour le Trainer de HuggingFace\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb40c52c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî® Cr√©ation des Datasets (Mode Lin√©aire pour Pneus)...\n"
     ]
    }
   ],
   "source": [
    "# ===================================================\n",
    "# CELLULE DE SAUVETAGE : INSTANCIATION & TRAINER\n",
    "# ===================================================\n",
    "from transformers import TrainingArguments\n",
    "import torch\n",
    "\n",
    "# 1. On recr√©e les Datasets pour √™tre S√õR qu'ils existent et sont bien configur√©s\n",
    "# Important : use_inverse=False pour la m√©trologie (mesure pr√©cise en mm)\n",
    "print(\"üî® Cr√©ation des Datasets (Mode Lin√©aire pour Pneus)...\")\n",
    "train_dataset = ZividDepthDataset(\n",
    "    image_folder=path_images,\n",
    "    depth_folder=path_depths,\n",
    "    image_files=train_imgs,\n",
    "    depth_files=train_depths,\n",
    "    processor=image_processor,\n",
    "    depth_min=GLOBAL_MIN,\n",
    "    depth_max=GLOBAL_MAX,\n",
    "    use_inverse=False \n",
    ")\n",
    "\n",
    "val_dataset = ZividDepthDataset(\n",
    "    image_folder=path_images,\n",
    "    depth_folder=path_depths,\n",
    "    image_files=val_imgs,\n",
    "    depth_files=val_depths,\n",
    "    processor=image_processor,\n",
    "    depth_min=GLOBAL_MIN,\n",
    "    depth_max=GLOBAL_MAX,\n",
    "    use_inverse=False\n",
    ")\n",
    "\n",
    "# 2. Nettoyage VRAM\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# 3. Arguments d'entra√Ænement\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./resultats_pneu_v5\",\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=1,      # Indispensable pour la haute r√©solution\n",
    "    gradient_accumulation_steps=8,      # Simule un batch de 8\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=True,\n",
    "    remove_unused_columns=False,\n",
    "    dataloader_num_workers=0,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 4. Initialisation du Trainer\n",
    "trainer = DepthTrainerFinal(\n",
    "    model=model_lora,                   # On utilise bien 'model_lora'\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,        # On utilise les variables cr√©√©es juste au-dessus\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=depth_data_collator,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1f5f4f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "üöÄ D√âMARRAGE IMM√âDIAT\n",
      "========================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/60 : < :, Epoch 0.17/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müöÄ D√âMARRAGE IMM√âDIAT\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m40\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 6. Sauvegarde\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model_lora\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./final_model_pneu_expert\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\accelerate\\accelerator.py:2852\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2850\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n\u001b[0;32m   2851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2852\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chikh\\anaconda4\\envs\\tns\\lib\\site-packages\\torch\\_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    625\u001b[0m     )\n\u001b[1;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chikh\\anaconda4\\envs\\tns\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\chikh\\anaconda4\\envs\\tns\\lib\\site-packages\\torch\\autograd\\graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    824\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    825\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5. Lancement !\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"üöÄ D√âMARRAGE IMM√âDIAT\")\n",
    "print(\"=\"*40)\n",
    "trainer.train()\n",
    "\n",
    "# 6. Sauvegarde\n",
    "model_lora.save_pretrained(\"./final_model_pneu_expert\")\n",
    "image_processor.save_pretrained(\"./final_model_pneu_expert\")\n",
    "print(\"‚úÖ Mod√®le sauvegard√© avec succ√®s !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83e3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# üìä VISUALISATION DES COURBES DE LOSS\n",
    "# ===================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# R√©cup√©ration de l'historique\n",
    "history = trainer.state.log_history\n",
    "\n",
    "# Extraction des donn√©es\n",
    "train_loss = [x['loss'] for x in history if 'loss' in x]\n",
    "eval_loss = [x['eval_loss'] for x in history if 'eval_loss' in x]\n",
    "epochs_train = [x['epoch'] for x in history if 'loss' in x]\n",
    "epochs_eval = [x['epoch'] for x in history if 'eval_loss' in x]\n",
    "\n",
    "# Trac√©\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(epochs_train, train_loss, label='Training Loss', color='blue', alpha=0.6)\n",
    "if len(eval_loss) > 0:\n",
    "    plt.plot(epochs_eval, eval_loss, label='Validation Loss', color='orange', linewidth=2)\n",
    "\n",
    "plt.title(\"√âvolution de l'apprentissage (Loss)\", fontsize=16)\n",
    "plt.xlabel(\"√âpoques\")\n",
    "plt.ylabel(\"Loss (Erreur)\")\n",
    "plt.legend()\n",
    "plt.grid(True, which='both', linestyle='--', alpha=0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Si la courbe orange descend et se stabilise, c'est gagn√© !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0350fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===================================================\n",
    "# üëÅÔ∏è R√âSULTAT VISUEL FINAL (RGB vs GT vs PRED)\n",
    "# ===================================================\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 1. S√©lectionner une image al√©atoire dans la Validation\n",
    "idx = random.randint(0, len(val_dataset) - 1)\n",
    "sample = val_dataset[idx]\n",
    "\n",
    "# Pr√©paration des donn√©es\n",
    "pixel_values = sample[\"pixel_values\"].unsqueeze(0).to(model_lora.device) # Ajout dimension batch\n",
    "label = sample[\"labels\"].cpu().numpy()\n",
    "\n",
    "# 2. Inf√©rence (Pr√©diction)\n",
    "model_lora.eval() # Mode √©valuation\n",
    "with torch.no_grad():\n",
    "    outputs = model_lora(pixel_values=pixel_values)\n",
    "    pred_raw = outputs.predicted_depth\n",
    "\n",
    "    # Remise √† l'√©chelle (Interpolation vers la taille de l'√©tiquette)\n",
    "    # Le mod√®le sort du 756x1260, on s'assure que √ßa matche le label\n",
    "    pred_interpolated = F.interpolate(\n",
    "        pred_raw.unsqueeze(1), \n",
    "        size=label.shape, \n",
    "        mode=\"bicubic\", \n",
    "        align_corners=False\n",
    "    ).squeeze().cpu().numpy()\n",
    "\n",
    "# 3. Affichage Comparatif\n",
    "fig, axs = plt.subplots(1, 3, figsize=(20, 10))\n",
    "\n",
    "# Image RGB (On doit d√©normaliser pour l'affichage si le processor a normalis√©, \n",
    "# mais ici on affiche l'image brute via le dataset ou on laisse matplotlib g√©rer)\n",
    "# Pour faire simple, on r√©ouvre l'image originale pour un affichage propre\n",
    "original_img_path = os.path.join(path_images, val_dataset.image_files[idx])\n",
    "original_img = Image.open(original_img_path)\n",
    "\n",
    "axs[0].imshow(original_img)\n",
    "axs[0].set_title(\"üì∏ Image Couleur (RGB)\", fontsize=14)\n",
    "axs[0].axis(\"off\")\n",
    "\n",
    "# V√©rit√© Terrain (Ground Truth)\n",
    "# On utilise 'magma' car c'est excellent pour la profondeur (Noir=Loin, Clair=Proche)\n",
    "d_min, d_max = label.min(), label.max()\n",
    "axs[1].imshow(label, cmap='magma', vmin=d_min, vmax=d_max)\n",
    "axs[1].set_title(\"üéØ V√©rit√© Terrain (Zivid)\", fontsize=14)\n",
    "axs[1].axis(\"off\")\n",
    "\n",
    "# Pr√©diction du Mod√®le\n",
    "axs[2].imshow(pred_interpolated, cmap='magma', vmin=d_min, vmax=d_max)\n",
    "axs[2].set_title(\"ü§ñ Pr√©diction (Depth Anything)\", fontsize=14)\n",
    "axs[2].axis(\"off\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úÖ Visualisation de l'√©chantillon n¬∞{idx}\")\n",
    "print(\"üëâ Regarde bien les rainures du pneu sur la 3√®me image !\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
