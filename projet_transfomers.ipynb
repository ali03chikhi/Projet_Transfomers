{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io\n",
        "from pathlib import Path\n",
        "import os\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from peft import LoraConfig, get_peft_model, TaskType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚è≥ Installation de PyTorch (version CUDA)...\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu122\n",
            "Requirement already satisfied: torch in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (2.9.1)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚è≥ Installation des outils d'image...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not find a version that satisfies the requirement torchvision (from versions: none)\n",
            "ERROR: No matching distribution found for torchvision\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (4.12.0.88)\n",
            "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (11.1.0)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
            "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚è≥ Installation de Transformers et Hugging Face...\n",
            "Requirement already satisfied: transformers in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (4.57.1)\n",
            "Requirement already satisfied: huggingface-hub in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (0.36.0)\n",
            "Requirement already satisfied: peft in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (1.11.0)\n",
            "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from huggingface-hub) (4.12.2)\n",
            "Requirement already satisfied: psutil in c:\\programdata\\anaconda3\\lib\\site-packages (from peft) (5.9.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in c:\\users\\abchikhi\\appdata\\roaming\\python\\python313\\site-packages (from peft) (2.9.1)\n",
            "Requirement already satisfied: sympy>=1.13.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (1.13.3)\n",
            "Requirement already satisfied: networkx>=2.5.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from torch>=1.13.0->peft) (72.1.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ Installation termin√©e. VEUILLEZ RED√âMARRER LE KERNEL.\n"
          ]
        }
      ],
      "source": [
        "# On utilise %pip pour cibler le bon environnement Python\n",
        "# On ajoute --user pour √©viter l'erreur \"Acc√®s refus√©\"\n",
        "\n",
        "print(\"‚è≥ Installation de PyTorch (version CUDA)...\")\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu122 --user\n",
        "\n",
        "print(\"‚è≥ Installation des outils d'image...\")\n",
        "%pip install opencv-python pillow numpy matplotlib tqdm --user\n",
        "\n",
        "print(\"‚è≥ Installation de Transformers et Hugging Face...\")\n",
        "%pip install transformers huggingface-hub peft accelerate --user\n",
        "\n",
        "print(\"‚úÖ Installation termin√©e. VEUILLEZ RED√âMARRER LE KERNEL.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Import r√©ussi !\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
        "print(\"‚úÖ Import r√©ussi !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1xn7dUHNYD9",
        "outputId": "77c4726d-2f5e-436a-a61f-553c0d07b149"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==================================================\n",
            "DETAILED CHANNEL ANALYSIS\n",
            "==================================================\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'depth_data' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Check each channel separately\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     channel \u001b[38;5;241m=\u001b[39m depth_data[:, :, i]\n\u001b[0;32m     10\u001b[0m     valid_count \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m~\u001b[39mnp\u001b[38;5;241m.\u001b[39misnan(channel))\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m     11\u001b[0m     nan_count \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39misnan(channel)\u001b[38;5;241m.\u001b[39msum()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'depth_data' is not defined"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "print(\"=\" * 50)\n",
        "print(\"DETAILED CHANNEL ANALYSIS\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Check each channel separately\n",
        "for i in range(3):\n",
        "    channel = depth_data[:, :, i]\n",
        "    valid_count = (~np.isnan(channel)).sum()\n",
        "    nan_count = np.isnan(channel).sum()\n",
        "\n",
        "    print(f\"\\nChannel {i}:\")\n",
        "    print(f\"  Valid values: {valid_count} ({valid_count / channel.size * 100:.2f}%)\")\n",
        "    print(f\"  NaN values: {nan_count} ({nan_count / channel.size * 100:.2f}%)\")\n",
        "\n",
        "    if valid_count > 0:\n",
        "        print(f\"  Min (ignoring NaN): {np.nanmin(channel):.2f}\")\n",
        "        print(f\"  Max (ignoring NaN): {np.nanmax(channel):.2f}\")\n",
        "        print(f\"  Mean (ignoring NaN): {np.nanmean(channel):.2f}\")\n",
        "\n",
        "# Overall statistics\n",
        "total_pixels = depth_data.shape[0] * depth_data.shape[1]\n",
        "total_values = depth_data.size\n",
        "valid_pixels_count = (~np.isnan(depth_data)).any(axis=2).sum()\n",
        "\n",
        "print(f\"\\n{'=' * 50}\")\n",
        "print(\"OVERALL STATISTICS\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"Total pixels: {total_pixels}\")\n",
        "print(f\"Pixels with at least one valid value: {valid_pixels_count} ({valid_pixels_count/total_pixels*100:.1f}%)\")\n",
        "print(f\"Total values (across 3 channels): {total_values}\")\n",
        "print(f\"Total valid values: {(~np.isnan(depth_data)).sum()}\")\n",
        "\n",
        "# Try to find and display some valid values if they exist\n",
        "print(f\"\\n{'=' * 50}\")\n",
        "print(\"SEARCHING FOR VALID DATA...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "valid_mask = ~np.isnan(depth_data)\n",
        "if valid_mask.any():\n",
        "    # Find pixels with valid data\n",
        "    valid_pixels = np.any(valid_mask, axis=2)\n",
        "    valid_coords = np.argwhere(valid_pixels)\n",
        "\n",
        "    if len(valid_coords) > 0:\n",
        "        print(f\"Found {len(valid_coords)} pixels with valid data!\")\n",
        "        print(f\"\\nFirst 3 valid pixels:\")\n",
        "        for idx in valid_coords[:3]:\n",
        "            y, x = idx\n",
        "            print(f\"  Position ({y}, {x}): {depth_data[y, x]}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  WARNING: NO VALID DATA FOUND IN ENTIRE FILE!\")\n",
        "    print(\"\\nPossible issues:\")\n",
        "    print(\"  1. File is corrupted\")\n",
        "    print(\"  2. Wrong file exported from Zivid\")\n",
        "    print(\"  3. Data needs special loading procedure\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmdTUqEFODAa"
      },
      "source": [
        "Preparation de l'env\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "-pCSGEFMREbk"
      },
      "outputs": [],
      "source": [
        "def load_pretrained_model(model_name=\"depth-anything/Depth-Anything-V2-Small-hf\"):\n",
        "    \"\"\"\n",
        "    Charge le mod√®le Depth Anything V2 pr√©-entra√Æn√© et son processeur d'images\n",
        "    \"\"\"\n",
        "    print(f\"üì• Chargement du mod√®le: {model_name}\")\n",
        "\n",
        "    image_processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    base_model = AutoModelForDepthEstimation.from_pretrained(model_name)\n",
        "\n",
        "    print(f\"‚úÖ Mod√®le charg√© avec succ√®s\")\n",
        "    return base_model, image_processor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289,
          "referenced_widgets": [
            "1e58d3730b684917b3be89d38cd28ed7",
            "48fed41b86484cc69e97afd7201b3f1d",
            "836a5991140b4555af8547aff5249c35",
            "fb78ae1d82fa4cb491d18cb4deb7c1df",
            "7734292198df4e0e9800bccaa2df8fa6",
            "7f848dfbe32b4328a8995eae51747ef5",
            "ee145691058c46708dd70c8e82c4659b",
            "92c720ebd5d340d18a85951dbd0caa26",
            "607f56a23c4a4d258a44568d80d8b6e0",
            "86e1b9eba1ce40c789fee7d975e6d808",
            "4e5148adce63409bad335d560b48c469",
            "ee3e596dd0bc409eb1881ae126249055",
            "c8c7361241d642aba28db4583568a43d",
            "6a2beca0ae69432290746120f97d8d9d",
            "076e2ab7dedf4ed8b75a60f1f3a4c182",
            "85960be4e08a418daff7764b81cfc1a5",
            "157d607e7a5d4ddd9ba3d89812e33617",
            "5aa91a828b5d457b86a960116ee87ccf",
            "70f8d58533414a13b0d24e5baf4d801b",
            "7300f79f002b45a59b181c4f3b89b604",
            "2a2389a8e9474a1c954fea1ef57ff827",
            "309e815f91bf4a5e903514e9ae19cf4b",
            "dc130e6335c0479c9a14f58346264929",
            "22ba49dd0e2d44ab87ca48ba3c1c2694",
            "5ffa3f78d94940a09d39d879db31afa0",
            "18f42814e26745dbbfdbe1ac5cf57298",
            "71dd44e393084109951e526ddae89efb",
            "cef818123e294b4bbeb1a0b02fa9165d",
            "26a5af69e89f449abf359097bd213a83",
            "383b0a4ac06f44a8a02dea81a6eb8967",
            "0c6eba7aa7de4611b9176572cb53b7fe",
            "63377198212c4fab8098723b7cd6915d",
            "8233d752f5e54588b9f565c34d1efd24"
          ]
        },
        "id": "tm4gWh0oR9og",
        "outputId": "dea719a9-c15f-449f-87ec-1d6e20199182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Chargement du mod√®le: depth-anything/Depth-Anything-V2-Small-hf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Mod√®le charg√© avec succ√®s\n"
          ]
        }
      ],
      "source": [
        "base_model, image_processor = load_pretrained_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "D0KEQcWmSRtZ"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "base_model = base_model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ESvNSUCKT-wz",
        "outputId": "384a2893-2cf8-40d3-c0c7-354813d703d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Total parameters: 24,785,089\n",
            "üìä Trainable parameters: 24,785,089\n"
          ]
        }
      ],
      "source": [
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in base_model.parameters())\n",
        "trainable_params = sum(p.numel() for p in base_model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f\"üìä Total parameters: {total_params:,}\")\n",
        "print(f\"üìä Trainable parameters: {trainable_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4g10AY5Rejk6",
        "outputId": "142e817e-3fd4-46d8-8586-9385fbef7af5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Inspecting model architecture...\n",
            "\n",
            "============================================================\n",
            "MODEL STRUCTURE:\n",
            "  : DepthAnythingForDepthEstimation\n",
            "  backbone: Dinov2Backbone\n",
            "  backbone.embeddings: Dinov2Embeddings\n",
            "  backbone.embeddings.patch_embeddings: Dinov2PatchEmbeddings\n",
            "  backbone.embeddings.patch_embeddings.projection: Conv2d\n",
            "  backbone.embeddings.dropout: Dropout\n",
            "  backbone.encoder: Dinov2Encoder\n",
            "  backbone.encoder.layer: ModuleList\n",
            "  backbone.encoder.layer.0: Dinov2Layer\n",
            "  backbone.encoder.layer.0.norm1: LayerNorm\n",
            "  backbone.encoder.layer.0.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.0.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.0.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.0.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.0.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.0.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.0.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.0.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.0.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.0.drop_path: Identity\n",
            "  backbone.encoder.layer.0.norm2: LayerNorm\n",
            "  backbone.encoder.layer.0.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.0.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.0.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.0.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.0.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.1: Dinov2Layer\n",
            "  backbone.encoder.layer.1.norm1: LayerNorm\n",
            "  backbone.encoder.layer.1.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.1.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.1.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.1.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.1.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.1.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.1.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.1.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.1.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.1.drop_path: Identity\n",
            "  backbone.encoder.layer.1.norm2: LayerNorm\n",
            "  backbone.encoder.layer.1.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.1.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.1.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.1.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.1.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.2: Dinov2Layer\n",
            "  backbone.encoder.layer.2.norm1: LayerNorm\n",
            "  backbone.encoder.layer.2.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.2.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.2.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.2.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.2.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.2.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.2.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.2.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.2.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.2.drop_path: Identity\n",
            "  backbone.encoder.layer.2.norm2: LayerNorm\n",
            "  backbone.encoder.layer.2.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.2.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.2.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.2.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.2.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.3: Dinov2Layer\n",
            "  backbone.encoder.layer.3.norm1: LayerNorm\n",
            "  backbone.encoder.layer.3.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.3.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.3.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.3.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.3.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.3.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.3.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.3.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.3.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.3.drop_path: Identity\n",
            "  backbone.encoder.layer.3.norm2: LayerNorm\n",
            "  backbone.encoder.layer.3.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.3.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.3.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.3.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.3.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.4: Dinov2Layer\n",
            "  backbone.encoder.layer.4.norm1: LayerNorm\n",
            "  backbone.encoder.layer.4.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.4.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.4.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.4.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.4.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.4.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.4.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.4.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.4.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.4.drop_path: Identity\n",
            "  backbone.encoder.layer.4.norm2: LayerNorm\n",
            "  backbone.encoder.layer.4.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.4.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.4.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.4.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.4.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.5: Dinov2Layer\n",
            "  backbone.encoder.layer.5.norm1: LayerNorm\n",
            "  backbone.encoder.layer.5.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.5.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.5.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.5.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.5.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.5.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.5.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.5.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.5.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.5.drop_path: Identity\n",
            "  backbone.encoder.layer.5.norm2: LayerNorm\n",
            "  backbone.encoder.layer.5.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.5.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.5.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.5.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.5.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.6: Dinov2Layer\n",
            "  backbone.encoder.layer.6.norm1: LayerNorm\n",
            "  backbone.encoder.layer.6.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.6.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.6.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.6.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.6.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.6.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.6.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.6.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.6.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.6.drop_path: Identity\n",
            "  backbone.encoder.layer.6.norm2: LayerNorm\n",
            "  backbone.encoder.layer.6.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.6.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.6.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.6.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.6.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.7: Dinov2Layer\n",
            "  backbone.encoder.layer.7.norm1: LayerNorm\n",
            "  backbone.encoder.layer.7.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.7.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.7.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.7.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.7.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.7.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.7.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.7.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.7.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.7.drop_path: Identity\n",
            "  backbone.encoder.layer.7.norm2: LayerNorm\n",
            "  backbone.encoder.layer.7.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.7.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.7.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.7.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.7.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.8: Dinov2Layer\n",
            "  backbone.encoder.layer.8.norm1: LayerNorm\n",
            "  backbone.encoder.layer.8.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.8.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.8.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.8.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.8.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.8.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.8.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.8.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.8.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.8.drop_path: Identity\n",
            "  backbone.encoder.layer.8.norm2: LayerNorm\n",
            "  backbone.encoder.layer.8.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.8.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.8.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.8.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.8.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.9: Dinov2Layer\n",
            "  backbone.encoder.layer.9.norm1: LayerNorm\n",
            "  backbone.encoder.layer.9.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.9.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.9.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.9.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.9.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.9.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.9.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.9.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.9.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.9.drop_path: Identity\n",
            "  backbone.encoder.layer.9.norm2: LayerNorm\n",
            "  backbone.encoder.layer.9.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.9.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.9.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.9.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.9.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.10: Dinov2Layer\n",
            "  backbone.encoder.layer.10.norm1: LayerNorm\n",
            "  backbone.encoder.layer.10.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.10.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.10.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.10.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.10.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.10.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.10.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.10.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.10.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.10.drop_path: Identity\n",
            "  backbone.encoder.layer.10.norm2: LayerNorm\n",
            "  backbone.encoder.layer.10.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.10.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.10.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.10.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.10.layer_scale2: Dinov2LayerScale\n",
            "  backbone.encoder.layer.11: Dinov2Layer\n",
            "  backbone.encoder.layer.11.norm1: LayerNorm\n",
            "  backbone.encoder.layer.11.attention: Dinov2Attention\n",
            "  backbone.encoder.layer.11.attention.attention: Dinov2SelfAttention\n",
            "  backbone.encoder.layer.11.attention.attention.query: Linear\n",
            "  backbone.encoder.layer.11.attention.attention.key: Linear\n",
            "  backbone.encoder.layer.11.attention.attention.value: Linear\n",
            "  backbone.encoder.layer.11.attention.output: Dinov2SelfOutput\n",
            "  backbone.encoder.layer.11.attention.output.dense: Linear\n",
            "  backbone.encoder.layer.11.attention.output.dropout: Dropout\n",
            "  backbone.encoder.layer.11.layer_scale1: Dinov2LayerScale\n",
            "  backbone.encoder.layer.11.drop_path: Identity\n",
            "  backbone.encoder.layer.11.norm2: LayerNorm\n",
            "  backbone.encoder.layer.11.mlp: Dinov2MLP\n",
            "  backbone.encoder.layer.11.mlp.fc1: Linear\n",
            "  backbone.encoder.layer.11.mlp.activation: GELUActivation\n",
            "  backbone.encoder.layer.11.mlp.fc2: Linear\n",
            "  backbone.encoder.layer.11.layer_scale2: Dinov2LayerScale\n",
            "  backbone.layernorm: LayerNorm\n",
            "  neck: DepthAnythingNeck\n",
            "  neck.reassemble_stage: DepthAnythingReassembleStage\n",
            "  neck.reassemble_stage.layers: ModuleList\n",
            "  neck.reassemble_stage.layers.0: DepthAnythingReassembleLayer\n",
            "  neck.reassemble_stage.layers.0.projection: Conv2d\n",
            "  neck.reassemble_stage.layers.0.resize: ConvTranspose2d\n",
            "  neck.reassemble_stage.layers.1: DepthAnythingReassembleLayer\n",
            "  neck.reassemble_stage.layers.1.projection: Conv2d\n",
            "  neck.reassemble_stage.layers.1.resize: ConvTranspose2d\n",
            "  neck.reassemble_stage.layers.2: DepthAnythingReassembleLayer\n",
            "  neck.reassemble_stage.layers.2.projection: Conv2d\n",
            "  neck.reassemble_stage.layers.2.resize: Identity\n",
            "  neck.reassemble_stage.layers.3: DepthAnythingReassembleLayer\n",
            "  neck.reassemble_stage.layers.3.projection: Conv2d\n",
            "  neck.reassemble_stage.layers.3.resize: Conv2d\n",
            "  neck.convs: ModuleList\n",
            "  neck.convs.0: Conv2d\n",
            "  neck.convs.1: Conv2d\n",
            "  neck.convs.2: Conv2d\n",
            "  neck.convs.3: Conv2d\n",
            "  neck.fusion_stage: DepthAnythingFeatureFusionStage\n",
            "  neck.fusion_stage.layers: ModuleList\n",
            "  neck.fusion_stage.layers.0: DepthAnythingFeatureFusionLayer\n",
            "  neck.fusion_stage.layers.0.projection: Conv2d\n",
            "  neck.fusion_stage.layers.0.residual_layer1: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.0.residual_layer1.activation1: ReLU\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.0.residual_layer1.activation2: ReLU\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.0.residual_layer2: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.0.residual_layer2.activation1: ReLU\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.0.residual_layer2.activation2: ReLU\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.1: DepthAnythingFeatureFusionLayer\n",
            "  neck.fusion_stage.layers.1.projection: Conv2d\n",
            "  neck.fusion_stage.layers.1.residual_layer1: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.1.residual_layer1.activation1: ReLU\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.1.residual_layer1.activation2: ReLU\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.1.residual_layer2: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.1.residual_layer2.activation1: ReLU\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.1.residual_layer2.activation2: ReLU\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.2: DepthAnythingFeatureFusionLayer\n",
            "  neck.fusion_stage.layers.2.projection: Conv2d\n",
            "  neck.fusion_stage.layers.2.residual_layer1: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.2.residual_layer1.activation1: ReLU\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.2.residual_layer1.activation2: ReLU\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.2.residual_layer2: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.2.residual_layer2.activation1: ReLU\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.2.residual_layer2.activation2: ReLU\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.3: DepthAnythingFeatureFusionLayer\n",
            "  neck.fusion_stage.layers.3.projection: Conv2d\n",
            "  neck.fusion_stage.layers.3.residual_layer1: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.3.residual_layer1.activation1: ReLU\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.3.residual_layer1.activation2: ReLU\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution2: Conv2d\n",
            "  neck.fusion_stage.layers.3.residual_layer2: DepthAnythingPreActResidualLayer\n",
            "  neck.fusion_stage.layers.3.residual_layer2.activation1: ReLU\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution1: Conv2d\n",
            "  neck.fusion_stage.layers.3.residual_layer2.activation2: ReLU\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution2: Conv2d\n",
            "  head: DepthAnythingDepthEstimationHead\n",
            "  head.conv1: Conv2d\n",
            "  head.conv2: Conv2d\n",
            "  head.activation1: ReLU\n",
            "  head.conv3: Conv2d\n",
            "  head.activation2: ReLU\n",
            "\n",
            "============================================================\n",
            "TRAINABLE LAYERS (before LoRA):\n",
            "  backbone.embeddings.cls_token: torch.Size([1, 1, 384])\n",
            "  backbone.embeddings.mask_token: torch.Size([1, 384])\n",
            "  backbone.embeddings.position_embeddings: torch.Size([1, 1370, 384])\n",
            "  backbone.embeddings.patch_embeddings.projection.weight: torch.Size([384, 3, 14, 14])\n",
            "  backbone.embeddings.patch_embeddings.projection.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.0.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.0.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.0.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.0.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.0.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.0.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.0.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.0.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.0.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.0.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.0.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.1.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.1.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.1.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.1.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.1.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.1.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.1.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.1.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.1.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.1.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.1.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.1.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.2.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.2.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.2.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.2.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.2.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.2.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.2.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.2.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.2.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.2.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.2.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.2.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.3.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.3.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.3.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.3.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.3.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.3.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.3.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.3.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.3.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.3.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.3.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.3.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.4.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.4.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.4.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.4.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.4.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.4.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.4.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.4.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.4.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.4.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.4.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.4.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.5.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.5.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.5.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.5.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.5.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.5.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.5.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.5.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.5.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.5.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.5.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.5.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.6.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.6.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.6.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.6.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.6.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.6.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.6.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.6.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.6.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.6.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.6.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.6.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.7.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.7.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.7.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.7.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.7.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.7.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.7.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.7.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.7.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.7.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.7.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.7.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.8.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.8.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.8.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.8.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.8.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.8.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.8.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.8.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.8.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.8.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.8.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.8.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.9.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.9.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.9.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.9.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.9.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.9.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.9.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.9.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.9.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.9.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.9.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.9.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.10.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.10.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.10.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.10.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.10.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.10.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.10.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.10.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.10.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.10.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.10.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.10.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.11.norm1.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.11.norm1.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.attention.attention.query.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.11.attention.attention.query.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.attention.attention.key.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.11.attention.attention.key.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.attention.attention.value.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.11.attention.attention.value.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.attention.output.dense.weight: torch.Size([384, 384])\n",
            "  backbone.encoder.layer.11.attention.output.dense.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.layer_scale1.lambda1: torch.Size([384])\n",
            "  backbone.encoder.layer.11.norm2.weight: torch.Size([384])\n",
            "  backbone.encoder.layer.11.norm2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.mlp.fc1.weight: torch.Size([1536, 384])\n",
            "  backbone.encoder.layer.11.mlp.fc1.bias: torch.Size([1536])\n",
            "  backbone.encoder.layer.11.mlp.fc2.weight: torch.Size([384, 1536])\n",
            "  backbone.encoder.layer.11.mlp.fc2.bias: torch.Size([384])\n",
            "  backbone.encoder.layer.11.layer_scale2.lambda1: torch.Size([384])\n",
            "  backbone.layernorm.weight: torch.Size([384])\n",
            "  backbone.layernorm.bias: torch.Size([384])\n",
            "  neck.reassemble_stage.layers.0.projection.weight: torch.Size([48, 384, 1, 1])\n",
            "  neck.reassemble_stage.layers.0.projection.bias: torch.Size([48])\n",
            "  neck.reassemble_stage.layers.0.resize.weight: torch.Size([48, 48, 4, 4])\n",
            "  neck.reassemble_stage.layers.0.resize.bias: torch.Size([48])\n",
            "  neck.reassemble_stage.layers.1.projection.weight: torch.Size([96, 384, 1, 1])\n",
            "  neck.reassemble_stage.layers.1.projection.bias: torch.Size([96])\n",
            "  neck.reassemble_stage.layers.1.resize.weight: torch.Size([96, 96, 2, 2])\n",
            "  neck.reassemble_stage.layers.1.resize.bias: torch.Size([96])\n",
            "  neck.reassemble_stage.layers.2.projection.weight: torch.Size([192, 384, 1, 1])\n",
            "  neck.reassemble_stage.layers.2.projection.bias: torch.Size([192])\n",
            "  neck.reassemble_stage.layers.3.projection.weight: torch.Size([384, 384, 1, 1])\n",
            "  neck.reassemble_stage.layers.3.projection.bias: torch.Size([384])\n",
            "  neck.reassemble_stage.layers.3.resize.weight: torch.Size([384, 384, 3, 3])\n",
            "  neck.reassemble_stage.layers.3.resize.bias: torch.Size([384])\n",
            "  neck.convs.0.weight: torch.Size([64, 48, 3, 3])\n",
            "  neck.convs.1.weight: torch.Size([64, 96, 3, 3])\n",
            "  neck.convs.2.weight: torch.Size([64, 192, 3, 3])\n",
            "  neck.convs.3.weight: torch.Size([64, 384, 3, 3])\n",
            "  neck.fusion_stage.layers.0.projection.weight: torch.Size([64, 64, 1, 1])\n",
            "  neck.fusion_stage.layers.0.projection.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.0.residual_layer1.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.0.residual_layer2.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.1.projection.weight: torch.Size([64, 64, 1, 1])\n",
            "  neck.fusion_stage.layers.1.projection.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.1.residual_layer1.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.1.residual_layer2.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.2.projection.weight: torch.Size([64, 64, 1, 1])\n",
            "  neck.fusion_stage.layers.2.projection.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.2.residual_layer1.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.2.residual_layer2.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.3.projection.weight: torch.Size([64, 64, 1, 1])\n",
            "  neck.fusion_stage.layers.3.projection.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.3.residual_layer1.convolution2.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution1.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution1.bias: torch.Size([64])\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution2.weight: torch.Size([64, 64, 3, 3])\n",
            "  neck.fusion_stage.layers.3.residual_layer2.convolution2.bias: torch.Size([64])\n",
            "  head.conv1.weight: torch.Size([32, 64, 3, 3])\n",
            "  head.conv1.bias: torch.Size([32])\n",
            "  head.conv2.weight: torch.Size([32, 32, 3, 3])\n",
            "  head.conv2.bias: torch.Size([32])\n",
            "  head.conv3.weight: torch.Size([1, 32, 1, 1])\n",
            "  head.conv3.bias: torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# INSPECT MODEL ARCHITECTURE\n",
        "# ===================================================\n",
        "print(\"üîç Inspecting model architecture...\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Print all module names\n",
        "print(\"MODEL STRUCTURE:\")\n",
        "for name, module in base_model.named_modules():\n",
        "    print(f\"  {name}: {module.__class__.__name__}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"TRAINABLE LAYERS (before LoRA):\")\n",
        "for name, param in base_model.named_parameters():\n",
        "    if param.requires_grad:\n",
        "        print(f\"  {name}: {param.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fy66WTOwgAVY"
      },
      "source": [
        "PREPARATION  des donn√©es"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "path_images = r\"P:\\images\"\n",
        "path_depths = r\"P:\\depth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Dossier Images vis√© : P:\\images\n",
            "üìÇ Dossier Depth vis√©  : P:\\depth\n",
            "\n",
            "‚úÖ Les dossiers existent !\n",
            "   -> Trouv√© 58 images\n",
            "   -> Trouv√© 58 fichiers XYZ (.npy)\n"
          ]
        }
      ],
      "source": [
        "print(f\"üìÇ Dossier Images vis√© : {path_images}\")\n",
        "print(f\"üìÇ Dossier Depth vis√©  : {path_depths}\")\n",
        "\n",
        "# V√©rification que les dossiers existent\n",
        "if os.path.exists(path_images) and os.path.exists(path_depths):\n",
        "    print(\"\\n‚úÖ Les dossiers existent !\")\n",
        "    \n",
        "    # Compter les fichiers pour √™tre s√ªr\n",
        "    nb_img = len([f for f in os.listdir(path_images) if f.endswith(('.png', '.jpg'))])\n",
        "    nb_npy = len([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
        "    \n",
        "    print(f\"   -> Trouv√© {nb_img} images\")\n",
        "    print(f\"   -> Trouv√© {nb_npy} fichiers XYZ (.npy)\")\n",
        "else:\n",
        "    print(\"\\n‚ùå ERREUR : Un des chemins est introuvable.\")\n",
        "    print(\"   V√©rifie l'orthographe ou copie le chemin absolu.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç D√©marrage de la v√©rification...\n",
            "üìä Bilan comptable :\n",
            "   - Images (.png/.jpg) : 58\n",
            "   - Depths (.npy)      : 58\n"
          ]
        }
      ],
      "source": [
        "# 2. COMPTAGE ET LISTING\n",
        "# ==========================================\n",
        "print(\"üîç D√©marrage de la v√©rification...\")\n",
        "\n",
        "# On liste et on trie pour garantir l'ordre\n",
        "# On ne garde que les fichiers (pas les dossiers cach√©s)\n",
        "files_img = sorted([f for f in os.listdir(path_images) if f.endswith(('.png', '.jpg'))])\n",
        "files_npy = sorted([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
        "\n",
        "num_img = len(files_img)\n",
        "num_npy = len(files_npy)\n",
        "\n",
        "print(f\"üìä Bilan comptable :\")\n",
        "print(f\"   - Images (.png/.jpg) : {num_img}\")\n",
        "print(f\"   - Depths (.npy)      : {num_npy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Le nombre de fichiers est identique.\n",
            "‚úÖ Tous les noms de fichiers semblent correspondre (logique color <-> rawDepth).\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. V√âRIFICATION DE L'ALIGNEMENT\n",
        "# ==========================================\n",
        "if num_img != num_npy:\n",
        "    print(f\"‚ùå ATTENTION : Le nombre de fichiers ne correspond pas ! (Diff√©rence : {abs(num_img - num_npy)})\")\n",
        "else:\n",
        "    print(\"‚úÖ Le nombre de fichiers est identique.\")\n",
        "\n",
        "# V√©rification des correspondances de noms (optionnel mais recommand√©)\n",
        "# On v√©rifie si image_01_color.png a bien son √©quivalent image_01_rawDepth.npy\n",
        "mismatches = []\n",
        "for img_name in files_img:\n",
        "    # On recr√©e le nom attendu du fichier depth (selon la logique Zivid vue dans ton notebook)\n",
        "    # Ex: \"scan_color.png\" -> \"scan_rawDepth.npy\"\n",
        "    expected_depth_name = img_name.replace(\"_color.png\", \"_rawDepth.npy\").replace(\".png\", \".npy\")\n",
        "    \n",
        "    # Si tes fichiers n'ont pas \"_color\", ajuste cette ligne (ex: replace(\".png\", \".npy\"))\n",
        "    \n",
        "    if expected_depth_name not in files_npy:\n",
        "        mismatches.append((img_name, expected_depth_name))\n",
        "\n",
        "if len(mismatches) > 0:\n",
        "    print(f\"‚ö†Ô∏è ATTENTION : {len(mismatches)} paires ne correspondent pas au niveau du nom !\")\n",
        "    print(f\"   Exemple de manquant : {mismatches[0]}\")\n",
        "else:\n",
        "    print(\"‚úÖ Tous les noms de fichiers semblent correspondre (logique color <-> rawDepth).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Test d'extraction du canal Z sur un fichier al√©atoire...\n",
            "   Fichier : 22-10-25-16-37-38_Zivid_acquisition_rawDepth.npy\n",
            "   Shape originale : (1200, 1944, 3)\n",
            "   ‚úÖ Extraction Z r√©ussie ! Shape finale : (1200, 1944)\n",
            "   Val Min (avec NaN) : 287.8849792480469\n",
            "   Val Max (avec NaN) : 2807.615478515625\n",
            "   Nombre de pixels NaN (vides) : 665725 (28.5%)\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 4. TEST D'EXTRACTION DU Z (Le fameux Z)\n",
        "# ==========================================\n",
        "\n",
        "if num_npy > 0:\n",
        "    print(\"\\nüß™ Test d'extraction du canal Z sur un fichier al√©atoire...\")\n",
        "    \n",
        "    # Prendre un fichier au hasard\n",
        "    random_npy = random.choice(files_npy)\n",
        "    full_path = os.path.join(path_depths, random_npy)\n",
        "    \n",
        "    try:\n",
        "        # Chargement\n",
        "        data = np.load(full_path) # Shape (H, W, 3) normalement\n",
        "        \n",
        "        print(f\"   Fichier : {random_npy}\")\n",
        "        print(f\"   Shape originale : {data.shape}\")\n",
        "        \n",
        "        if len(data.shape) == 3 and data.shape[2] == 3:\n",
        "            # EXTRACTION DU Z (Canal index 2)\n",
        "            Z_channel = data[:, :, 2]\n",
        "            \n",
        "            # V√©rif statistiques\n",
        "            print(f\"   ‚úÖ Extraction Z r√©ussie ! Shape finale : {Z_channel.shape}\")\n",
        "            print(f\"   Val Min (avec NaN) : {np.nanmin(Z_channel)}\")\n",
        "            print(f\"   Val Max (avec NaN) : {np.nanmax(Z_channel)}\")\n",
        "            \n",
        "            # V√©rification des NaN\n",
        "            nan_count = np.isnan(Z_channel).sum()\n",
        "            print(f\"   Nombre de pixels NaN (vides) : {nan_count} ({(nan_count/Z_channel.size)*100:.1f}%)\")\n",
        "        else:\n",
        "            print(\"‚ùå Erreur : Le fichier .npy n'a pas 3 canaux (XYZ). V√©rifie le format.\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Erreur lors du chargement : {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Le fichier charg√© est de forme : (1200, 1944)\n",
            "La valeur de Profondeur (Z) au pixel [600, 972] est :\n",
            "-> **1365.02 mm**\n"
          ]
        }
      ],
      "source": [
        "filename = \"21-12-03-19-05-46_Zivid_acquisition_rawDepth.npy\"\n",
        "full_path = os.path.join(path_depths, filename)\n",
        "\n",
        "# --- EXTRACTION ---\n",
        "# 1. Chargement de la donn√©e (1200, 1944, 3)\n",
        "data_xyz = np.load(full_path)\n",
        "\n",
        "# 2. Extraction de la carte de profondeur Z (1200, 1944)\n",
        "Z_channel = data_xyz[:, :, 2] \n",
        "\n",
        "# 3. Traitement des NaN (pour s'assurer qu'on peut lire)\n",
        "Z_clean = np.nan_to_num(Z_channel, nan=0.0)\n",
        "\n",
        "# --- LECTURE DE LA VALEUR D'UN PIXEL ---\n",
        "pixel_row = 600\n",
        "pixel_col = 972\n",
        "\n",
        "# Acc√©der √† la valeur Z du pixel (600, 972)\n",
        "profondeur_pixel = Z_clean[pixel_row, pixel_col]\n",
        "\n",
        "print(f\"Le fichier charg√© est de forme : {Z_channel.shape}\")\n",
        "print(f\"La valeur de Profondeur (Z) au pixel [{pixel_row}, {pixel_col}] est :\")\n",
        "print(f\"-> **{profondeur_pixel:.2f} mm**\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Nous avons choisi de traiter les donn√©es (extraire le canal Z et remplacer les NaN par 0.0) dans la fonction __getitem__ de la classe Dataset (c'est-√†-dire pendant l'entra√Ænement) au lieu de le faire une seule fois en amont.1. Efficacit√© et √âconomie de Stockage (Le Plus Important) Votre jeu de donn√©es est lourd, et l'objectif est d'√©viter de cr√©er des doublons :Donn√©es Brutes : Chaque fichier .npy stocke 3 canaux (X, Y, Z).Donn√©es Cible : Nous n'avons besoin que d'1 canal (Z).Si nous avions extrait Z et cr√©√© un nouveau fichier .npy pour chaque image :Vous auriez doubl√© le nombre de fichiers sur votre disque dur (un dossier images, un dossier depth_xyz, et un nouveau dossier depth_z_seul).Bien que le fichier de profondeur Z soit plus petit que le fichier XYZ, vous auriez pass√© un temps consid√©rable √† g√©n√©rer et stocker des milliers de nouveaux fichiers.En faisant l'op√©ration √† la vol√©e, le processus est : Lire le fichier original (XYZ) $\\rightarrow$ Extraire Z en m√©moire RAM $\\rightarrow$ Transf√©rer vers le GPU. Aucune duplication n'est n√©cessaire sur le disque."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Configuration de LoRA\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# --- 3. D√âFINITION DE LA CONFIGURATION LoRA ---\n",
        "\n",
        "# Pour les Vision Transformers, nous ciblons les couches d'attention (Query, Key, Value)\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                                    # Le rang de la matrice LoRA (ajustement de la capacit√© d'apprentissage)\n",
        "    lora_alpha=32,                           # Facteur de mise √† l'√©chelle (g√©n√©ralement 2 * r)\n",
        "    target_modules=[\"query\", \"key\", \"value\"],# Les couches sp√©cifiques du ViT √† modifier (Attention)\n",
        "    lora_dropout=0.05,                       # Dropout appliqu√© aux couches LoRA\n",
        "    bias=\"none\",                             # Ne pas adapter les biais\n",
        "    task_type=TaskType.FEATURE_EXTRACTION,   # Le ViT sert d'extracteur de features avant la t√™te de profondeur\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Mod√®le LoRA appliqu√© !\n",
            "trainable params: 442,368 || all params: 25,227,457 || trainable%: 1.7535\n"
          ]
        }
      ],
      "source": [
        "# La fonction get_peft_model g√®le les poids du mod√®le de base et ajoute les adaptateurs LoRA\n",
        "model_lora = get_peft_model(base_model, lora_config)\n",
        "\n",
        "print(\"‚úÖ Mod√®le LoRA appliqu√© !\")\n",
        "# Afficher le nombre de param√®tres entra√Æn√©s\n",
        "model_lora.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C'est ici que l'extraction et le nettoyage √† la vol√©e se produisent\n",
        "def __getitem__(self, idx):\n",
        "    # ... chargement de l'image ...\n",
        "    \n",
        "    # 1. Chargement du fichier complet (lent)\n",
        "    depth_data = np.load(depth_path) \n",
        "    \n",
        "    # 2. Extraction du Z et nettoyage des NaN (ultra-rapide)\n",
        "    z_channel = depth_data[:, :, 2]\n",
        "    z_channel = np.nan_to_num(z_channel, nan=0.0) \n",
        "    \n",
        "    # 3. Le processeur le pr√©pare pour le GPU\n",
        "    # ...\n",
        "    return { ... }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Calcul des statistiques sur TOUS les fichiers depth...\n",
            "   (Cela peut prendre quelques minutes...)\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Analyse des fichiers depth: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [00:17<00:00,  3.38it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STATISTIQUES GLOBALES DU DATASET\n",
            "============================================================\n",
            "üìè Profondeur MIN (sur tous les fichiers): 251.74 mm\n",
            "üìè Profondeur MAX (sur tous les fichiers): 3907.45 mm\n",
            "üìä Moyenne des moyennes: 1542.16 mm\n",
            "üìä √âcart-type: 295.35 mm\n",
            "‚úÖ Pixels valides (moyenne): 68.5%\n",
            "============================================================\n",
            "\n",
            "üí° Utilisez ces valeurs dans votre Dataset:\n",
            "   depth_min = 251.74\n",
            "   depth_max = 3907.45\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# CALCUL DES STATISTIQUES GLOBALES DU DATASET\n",
        "# ===================================================\n",
        "print(\"üìä Calcul des statistiques sur TOUS les fichiers depth...\")\n",
        "print(\"   (Cela peut prendre quelques minutes...)\\n\")\n",
        "\n",
        "depth_stats = {\n",
        "    'min_values': [],\n",
        "    'max_values': [],\n",
        "    'mean_values': [],\n",
        "    'valid_percentages': []\n",
        "}\n",
        "\n",
        "# Parcourir TOUS les fichiers .npy\n",
        "from tqdm import tqdm  # Pour une barre de progression\n",
        "\n",
        "for depth_file in tqdm(files_npy, desc=\"Analyse des fichiers depth\"):\n",
        "    depth_path = os.path.join(path_depths, depth_file)\n",
        "    \n",
        "    # Charger et extraire Z\n",
        "    depth_xyz = np.load(depth_path)\n",
        "    depth_z = depth_xyz[:, :, 2]\n",
        "    \n",
        "    # Calculer statistiques (en ignorant NaN)\n",
        "    if not np.all(np.isnan(depth_z)):  # Si au moins une valeur valide\n",
        "        depth_stats['min_values'].append(np.nanmin(depth_z))\n",
        "        depth_stats['max_values'].append(np.nanmax(depth_z))\n",
        "        depth_stats['mean_values'].append(np.nanmean(depth_z))\n",
        "        \n",
        "        # % de pixels valides\n",
        "        valid_pct = (~np.isnan(depth_z)).sum() / depth_z.size * 100\n",
        "        depth_stats['valid_percentages'].append(valid_pct)\n",
        "\n",
        "# Calculer les statistiques globales\n",
        "GLOBAL_MIN = np.min(depth_stats['min_values'])\n",
        "GLOBAL_MAX = np.max(depth_stats['max_values'])\n",
        "GLOBAL_MEAN = np.mean(depth_stats['mean_values'])\n",
        "GLOBAL_STD = np.std(depth_stats['mean_values'])\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STATISTIQUES GLOBALES DU DATASET\")\n",
        "print(\"=\"*60)\n",
        "print(f\"üìè Profondeur MIN (sur tous les fichiers): {GLOBAL_MIN:.2f} mm\")\n",
        "print(f\"üìè Profondeur MAX (sur tous les fichiers): {GLOBAL_MAX:.2f} mm\")\n",
        "print(f\"üìä Moyenne des moyennes: {GLOBAL_MEAN:.2f} mm\")\n",
        "print(f\"üìä √âcart-type: {GLOBAL_STD:.2f} mm\")\n",
        "print(f\"‚úÖ Pixels valides (moyenne): {np.mean(depth_stats['valid_percentages']):.1f}%\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Ces valeurs vont servir pour la normalisation!\n",
        "print(f\"\\nüí° Utilisez ces valeurs dans votre Dataset:\")\n",
        "print(f\"   depth_min = {GLOBAL_MIN:.2f}\")\n",
        "print(f\"   depth_max = {GLOBAL_MAX:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Classe Dataset (v2) d√©finie avec normalisation!\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# DATASET AVEC NORMALISATION CORRECTE\n",
        "# ===================================================\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "class ZividDepthDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset pour charger les paires RGB-Depth Zivid\n",
        "    \"\"\"\n",
        "    def __init__(self, image_folder, depth_folder, image_files, depth_files, \n",
        "                 processor, depth_min, depth_max):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_folder: Chemin vers dossier images\n",
        "            depth_folder: Chemin vers dossier depth\n",
        "            image_files: Liste des noms de fichiers images\n",
        "            depth_files: Liste des noms de fichiers depth\n",
        "            processor: ImageProcessor de Hugging Face\n",
        "            depth_min: Valeur minimale de profondeur (en mm)\n",
        "            depth_max: Valeur maximale de profondeur (en mm)\n",
        "        \"\"\"\n",
        "        self.image_folder = image_folder\n",
        "        self.depth_folder = depth_folder\n",
        "        self.image_files = image_files\n",
        "        self.depth_files = depth_files\n",
        "        self.processor = processor\n",
        "        self.depth_min = depth_min\n",
        "        self.depth_max = depth_max\n",
        "        \n",
        "        print(f\"üìä Dataset cr√©√©:\")\n",
        "        print(f\"   √âchantillons: {len(self.image_files)}\")\n",
        "        print(f\"   Normalisation: [{depth_min:.2f}, {depth_max:.2f}] mm ‚Üí [0, 1]\")\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.image_files)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        # 1. Charger l'image RGB\n",
        "        img_path = os.path.join(self.image_folder, self.image_files[idx])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        \n",
        "        # 2. Charger le fichier depth XYZ\n",
        "        depth_path = os.path.join(self.depth_folder, self.depth_files[idx])\n",
        "        depth_xyz = np.load(depth_path)\n",
        "        \n",
        "        # 3. Extraire le canal Z (profondeur)\n",
        "        depth_z = depth_xyz[:, :, 2]  # Shape: (1200, 1944)\n",
        "        \n",
        "        # 4. Cr√©er un masque des pixels valides (pour la loss)\n",
        "        valid_mask = ~np.isnan(depth_z)\n",
        "        \n",
        "        # 5. Remplacer NaN par 0 temporairement\n",
        "        depth_z_clean = np.nan_to_num(depth_z, nan=0.0)\n",
        "        \n",
        "        # 6. Normaliser la profondeur entre 0 et 1\n",
        "        depth_normalized = (depth_z_clean - self.depth_min) / (self.depth_max - self.depth_min)\n",
        "        depth_normalized = np.clip(depth_normalized, 0, 1)\n",
        "        \n",
        "        # 7. Preprocessing de l'image\n",
        "        inputs = self.processor(images=image, return_tensors=\"pt\")\n",
        "        \n",
        "        # 8. Retourner tout\n",
        "        return {\n",
        "            'pixel_values': inputs['pixel_values'].squeeze(0),\n",
        "            'depth_gt': torch.from_numpy(depth_normalized).float(),\n",
        "            'valid_mask': torch.from_numpy(valid_mask).float(),\n",
        "            'filename': self.image_files[idx]\n",
        "        }\n",
        "\n",
        "print(\"‚úÖ Classe Dataset (v2) d√©finie avec normalisation!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Total de fichiers: 58\n",
            "‚úÖ Split effectu√©:\n",
            "   Train: 46 √©chantillons\n",
            "   Val:   12 √©chantillons\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# SPLIT TRAIN / VALIDATION\n",
        "# ===================================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Vos fichiers d√©j√† tri√©s\n",
        "files_img = sorted([f for f in os.listdir(path_images) if f.endswith('.png')])\n",
        "files_npy = sorted([f for f in os.listdir(path_depths) if f.endswith('.npy')])\n",
        "\n",
        "print(f\"üìä Total de fichiers: {len(files_img)}\")\n",
        "\n",
        "# Split 80% train, 20% validation\n",
        "train_imgs, val_imgs, train_depths, val_depths = train_test_split(\n",
        "    files_img, files_npy, \n",
        "    test_size=0.2,  # 20% validation\n",
        "    random_state=42,  # Pour reproductibilit√©\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Split effectu√©:\")\n",
        "print(f\"   Train: {len(train_imgs)} √©chantillons\")\n",
        "print(f\"   Val:   {len(val_imgs)} √©chantillons\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Dataset cr√©√©:\n",
            "   √âchantillons: 46\n",
            "   Normalisation: [251.74, 3907.45] mm ‚Üí [0, 1]\n",
            "üìä Dataset cr√©√©:\n",
            "   √âchantillons: 12\n",
            "   Normalisation: [251.74, 3907.45] mm ‚Üí [0, 1]\n",
            "‚úÖ DataLoaders cr√©√©s!\n",
            "   Train batches: 23\n",
            "   Val batches: 6\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# CR√âATION DES DATALOADERS\n",
        "# ===================================================\n",
        "\n",
        "# Cr√©er les datasets\n",
        "train_dataset = ZividDepthDataset(\n",
        "    image_folder=path_images,\n",
        "    depth_folder=path_depths,\n",
        "    image_files=train_imgs,\n",
        "    depth_files=train_depths,\n",
        "    processor=image_processor,\n",
        "    depth_min = 251.74,\n",
        "   depth_max = 3907.45\n",
        ")\n",
        "\n",
        "val_dataset = ZividDepthDataset(\n",
        "    image_folder=path_images,\n",
        "    depth_folder=path_depths,\n",
        "    image_files=val_imgs,\n",
        "    depth_files=val_depths,\n",
        "    processor=image_processor,\n",
        "    depth_min = 251.74,\n",
        "   depth_max = 3907.45\n",
        ")\n",
        "\n",
        "# Cr√©er les dataloaders\n",
        "BATCH_SIZE = 2  # Petit batch car images volumineuses\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  # M√©langer pour l'entra√Ænement\n",
        "    num_workers=0,  # 0 pour Windows, 2-4 sinon\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,  # Pas de m√©lange pour validation\n",
        "    num_workers=0,\n",
        "    pin_memory=True if torch.cuda.is_available() else False\n",
        ")\n",
        "\n",
        "print(\"‚úÖ DataLoaders cr√©√©s!\")\n",
        "print(f\"   Train batches: {len(train_loader)}\")\n",
        "print(f\"   Val batches: {len(val_loader)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üß™ Test du DataLoader...\n",
            "‚úÖ Batch charg√© avec succ√®s!\n",
            "   pixel_values shape: torch.Size([2, 3, 518, 840])\n",
            "   depth_gt shape: torch.Size([2, 1200, 1944])\n",
            "   valid_mask shape: torch.Size([2, 1200, 1944])\n",
            "   Fichiers: ['21-12-03-18-52-52_Zivid_acquisition_color.png', '21-12-03-18-52-40_Zivid_acquisition_color.png']\n",
            "\n",
            "üìä Statistiques:\n",
            "   Depth GT min: 0.000\n",
            "   Depth GT max: 0.732\n",
            "   Pixels valides: 3184454.0 / 4665600\n"
          ]
        }
      ],
      "source": [
        "# ===================================================\n",
        "# TEST DU DATALOADER\n",
        "# ===================================================\n",
        "print(\"\\nüß™ Test du DataLoader...\")\n",
        "\n",
        "# R√©cup√©rer un batch\n",
        "batch = next(iter(train_loader))\n",
        "\n",
        "print(f\"‚úÖ Batch charg√© avec succ√®s!\")\n",
        "print(f\"   pixel_values shape: {batch['pixel_values'].shape}\")\n",
        "print(f\"   depth_gt shape: {batch['depth_gt'].shape}\")\n",
        "print(f\"   valid_mask shape: {batch['valid_mask'].shape}\")\n",
        "print(f\"   Fichiers: {batch['filename']}\")\n",
        "\n",
        "# V√©rifier les valeurs\n",
        "print(f\"\\nüìä Statistiques:\")\n",
        "print(f\"   Depth GT min: {batch['depth_gt'].min():.3f}\")\n",
        "print(f\"   Depth GT max: {batch['depth_gt'].max():.3f}\")\n",
        "print(f\"   Pixels valides: {batch['valid_mask'].sum().item()} / {batch['valid_mask'].numel()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "3. Initialisation du Trainer...\n"
          ]
        }
      ],
      "source": [
        "# ==========================================\n",
        "# 3. D√âFINITION DES ARGUMENTS ET DU TRAINER\n",
        "# ==========================================\n",
        "from transformers import IntervalStrategy,TrainingArguments, Trainer\n",
        "print(\"\\n3. Initialisation du Trainer...\")\n",
        "OUTPUT_DIR = \"./depth_anything_finetuned_lora_zivid\" \n",
        "BATCH_SIZE = 4 # ‚ö†Ô∏è IMPORTANT: R√©duire cette valeur (2 ou 1) si votre GPU manque de VRAM\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    learning_rate=5e-5,               \n",
        "    num_train_epochs=5,               \n",
        "    per_device_train_batch_size=BATCH_SIZE,      \n",
        "    per_device_eval_batch_size=BATCH_SIZE,              \n",
        "    eval_steps=500,                                  \n",
        "    save_steps=500,     \n",
        "    # Reste inchang√© :\n",
        "    logging_steps=50,     \n",
        "    report_to=\"tensorboard\",          \n",
        "    save_total_limit=3,               \n",
        "    fp16=True,                        \n",
        "    remove_unused_columns=False,        \n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tensorboard\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting absl-py>=0.4 (from tensorboard)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting grpcio>=1.48.2 (from tensorboard)\n",
            "  Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (3.8)\n",
            "Requirement already satisfied: numpy>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (2.1.3)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (24.2)\n",
            "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (11.1.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (5.29.3)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (72.1.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in c:\\programdata\\anaconda3\\lib\\site-packages (from grpcio>=1.48.2->tensorboard) (4.12.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard) (3.0.2)\n",
            "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 5.5/5.5 MB 49.3 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading grpcio-1.76.0-cp313-cp313-win_amd64.whl (4.7 MB)\n",
            "   ---------------------------------------- 0.0/4.7 MB ? eta -:--:--\n",
            "   ---------------------------------------- 4.7/4.7 MB 87.8 MB/s eta 0:00:00\n",
            "Installing collected packages: tensorboard-data-server, grpcio, absl-py, tensorboard\n",
            "\n",
            "   ---------- ----------------------------- 1/4 [grpcio]\n",
            "   ---------- ----------------------------- 1/4 [grpcio]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ------------------------------ --------- 3/4 [tensorboard]\n",
            "   ---------------------------------------- 4/4 [tensorboard]\n",
            "\n",
            "Successfully installed absl-py-2.3.1 grpcio-1.76.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\abchikhi\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Data collator explicite mis √† jour.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data._utils.collate import default_collate\n",
        "\n",
        "def depth_data_collator(features):\n",
        "    \"\"\"\n",
        "    Utilise le collate par d√©faut de PyTorch apr√®s avoir s'√™tre assur√©\n",
        "    que les features sont bien des dictionnaires avec les cl√©s 'pixel_values' et 'labels'.\n",
        "    \"\"\"\n",
        "    \n",
        "    # Le collator par d√©faut g√®re l'empilement de tenseurs\n",
        "    # On lui passe une liste de dictionnaires\n",
        "    batch = default_collate(features)\n",
        "    \n",
        "    # On s'assure que seules les cl√©s essentielles sont pr√©sentes (pixel_values et labels)\n",
        "    # Dans les versions r√©centes, le collator par d√©faut de PyTorch/Hugging Face\n",
        "    # peut inclure des cl√©s non pertinentes. Pour simplifier, on prend juste ce dont on a besoin.\n",
        "    \n",
        "    return {\n",
        "        \"pixel_values\": batch[\"pixel_values\"],\n",
        "        \"labels\": batch[\"labels\"]\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Data collator explicite mis √† jour.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üöÄ Trainer initialis√© avec le Data Collator corrig√©.\n"
          ]
        },
        {
          "ename": "KeyError",
          "evalue": "'labels'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[30], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124müöÄ Trainer initialis√© avec le Data Collator corrig√©.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# LANCEMENT !\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2326\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2327\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2328\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2329\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2330\u001b[0m     )\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:2618\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2616\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2617\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2618\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_batch_samples(epoch_iterator, num_batches, args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   2619\u001b[0m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[0;32m   2620\u001b[0m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[0;32m   2621\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:5654\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5652\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5654\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mnext\u001b[39m(epoch_iterator))\n\u001b[0;32m   5655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5656\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\accelerate\\data_loader.py:567\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(dataloader_iter)\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mend()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    738\u001b[0m ):\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
            "Cell \u001b[1;32mIn[29], line 20\u001b[0m, in \u001b[0;36mdepth_data_collator\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m     12\u001b[0m batch \u001b[38;5;241m=\u001b[39m default_collate(features)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# On s'assure que seules les cl√©s essentielles sont pr√©sentes (pixel_values et labels)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Dans les versions r√©centes, le collator par d√©faut de PyTorch/Hugging Face\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# peut inclure des cl√©s non pertinentes. Pour simplifier, on prend juste ce dont on a besoin.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m     21\u001b[0m }\n",
            "\u001b[1;31mKeyError\u001b[0m: 'labels'"
          ]
        }
      ],
      "source": [
        "# Initialisation du Trainer (Corrig√©e)\n",
        "trainer = Trainer(\n",
        "    model=model_lora,  \n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    #compute_metrics=compute_metrics,\n",
        "    \n",
        "    # üí• Ajout de l'argument data_collator\n",
        "    data_collator=depth_data_collator, \n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Trainer initialis√© avec le Data Collator corrig√©.\")\n",
        "\n",
        "# LANCEMENT !\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "076e2ab7dedf4ed8b75a60f1f3a4c182": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a2389a8e9474a1c954fea1ef57ff827",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_309e815f91bf4a5e903514e9ae19cf4b",
            "value": "‚Äá950/950‚Äá[00:00&lt;00:00,‚Äá54.3kB/s]"
          }
        },
        "0c6eba7aa7de4611b9176572cb53b7fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "157d607e7a5d4ddd9ba3d89812e33617": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "18f42814e26745dbbfdbe1ac5cf57298": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63377198212c4fab8098723b7cd6915d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_8233d752f5e54588b9f565c34d1efd24",
            "value": "‚Äá99.2M/99.2M‚Äá[00:02&lt;00:00,‚Äá53.1MB/s]"
          }
        },
        "1e58d3730b684917b3be89d38cd28ed7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48fed41b86484cc69e97afd7201b3f1d",
              "IPY_MODEL_836a5991140b4555af8547aff5249c35",
              "IPY_MODEL_fb78ae1d82fa4cb491d18cb4deb7c1df"
            ],
            "layout": "IPY_MODEL_7734292198df4e0e9800bccaa2df8fa6"
          }
        },
        "22ba49dd0e2d44ab87ca48ba3c1c2694": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cef818123e294b4bbeb1a0b02fa9165d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_26a5af69e89f449abf359097bd213a83",
            "value": "model.safetensors:‚Äá100%"
          }
        },
        "26a5af69e89f449abf359097bd213a83": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2a2389a8e9474a1c954fea1ef57ff827": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "309e815f91bf4a5e903514e9ae19cf4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "383b0a4ac06f44a8a02dea81a6eb8967": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48fed41b86484cc69e97afd7201b3f1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f848dfbe32b4328a8995eae51747ef5",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_ee145691058c46708dd70c8e82c4659b",
            "value": "preprocessor_config.json:‚Äá100%"
          }
        },
        "4e5148adce63409bad335d560b48c469": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aa91a828b5d457b86a960116ee87ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ffa3f78d94940a09d39d879db31afa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_383b0a4ac06f44a8a02dea81a6eb8967",
            "max": 99173660,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0c6eba7aa7de4611b9176572cb53b7fe",
            "value": 99173660
          }
        },
        "607f56a23c4a4d258a44568d80d8b6e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63377198212c4fab8098723b7cd6915d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a2beca0ae69432290746120f97d8d9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70f8d58533414a13b0d24e5baf4d801b",
            "max": 950,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7300f79f002b45a59b181c4f3b89b604",
            "value": 950
          }
        },
        "70f8d58533414a13b0d24e5baf4d801b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71dd44e393084109951e526ddae89efb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7300f79f002b45a59b181c4f3b89b604": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7734292198df4e0e9800bccaa2df8fa6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f848dfbe32b4328a8995eae51747ef5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8233d752f5e54588b9f565c34d1efd24": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "836a5991140b4555af8547aff5249c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92c720ebd5d340d18a85951dbd0caa26",
            "max": 775,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_607f56a23c4a4d258a44568d80d8b6e0",
            "value": 775
          }
        },
        "85960be4e08a418daff7764b81cfc1a5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86e1b9eba1ce40c789fee7d975e6d808": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92c720ebd5d340d18a85951dbd0caa26": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c8c7361241d642aba28db4583568a43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_157d607e7a5d4ddd9ba3d89812e33617",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5aa91a828b5d457b86a960116ee87ccf",
            "value": "config.json:‚Äá100%"
          }
        },
        "cef818123e294b4bbeb1a0b02fa9165d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc130e6335c0479c9a14f58346264929": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22ba49dd0e2d44ab87ca48ba3c1c2694",
              "IPY_MODEL_5ffa3f78d94940a09d39d879db31afa0",
              "IPY_MODEL_18f42814e26745dbbfdbe1ac5cf57298"
            ],
            "layout": "IPY_MODEL_71dd44e393084109951e526ddae89efb"
          }
        },
        "ee145691058c46708dd70c8e82c4659b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ee3e596dd0bc409eb1881ae126249055": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8c7361241d642aba28db4583568a43d",
              "IPY_MODEL_6a2beca0ae69432290746120f97d8d9d",
              "IPY_MODEL_076e2ab7dedf4ed8b75a60f1f3a4c182"
            ],
            "layout": "IPY_MODEL_85960be4e08a418daff7764b81cfc1a5"
          }
        },
        "fb78ae1d82fa4cb491d18cb4deb7c1df": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86e1b9eba1ce40c789fee7d975e6d808",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_4e5148adce63409bad335d560b48c469",
            "value": "‚Äá775/775‚Äá[00:00&lt;00:00,‚Äá36.4kB/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
