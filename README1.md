# ğŸ¯ Projet Devoir : Fine-Tuning de Depth Anything avec LoRA

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.30+-FFD21E?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

</div>

---

## ğŸ‘¥ Auteurs

| Nom                  |
| -------------------- | 
| **Abdelali Chikhi** | 
| **Ayman Zejli**      | 
| **Mouad Azenag**      | 
| **Loic Magnan**      | 

---

## ğŸ“– Contexte et Objectif

### Contexte

L'estimation de profondeur monoculaire est une tÃ¢che fondamentale en vision par ordinateur qui consiste Ã  prÃ©dire la distance des objets dans une scÃ¨ne Ã  partir d'une seule image RGB. Cette capacitÃ© est cruciale pour de nombreuses applications :

- ğŸš— **VÃ©hicules autonomes** : Navigation et Ã©vitement d'obstacles
- ğŸ¤– **Robotique** : Manipulation d'objets et navigation
- ğŸ­ **Industrie 4.0** : ContrÃ´le qualitÃ© et inspection automatisÃ©e
- ğŸ® **RÃ©alitÃ© augmentÃ©e** : Placement prÃ©cis d'objets virtuels

### Objectif du Projet

Ce projet vise Ã  **adapter le modÃ¨le Depth Anything** (un modÃ¨le prÃ©-entraÃ®nÃ© de pointe pour l'estimation de profondeur) au **jeu de donnÃ©es Zivid** spÃ©cifique Ã  un contexte industriel, en utilisant la technique de **LoRA (Low-Rank Adaptation)** pour un fine-tuning efficace.

#### Pourquoi LoRA ?

- âœ… RÃ©duction drastique des paramÃ¨tres entraÃ®nables (~1.75% des paramÃ¨tres totaux)
- âœ… PrÃ©servation des connaissances du modÃ¨le prÃ©-entraÃ®nÃ©
- âœ… EntraÃ®nement rapide avec moins de ressources GPU
- âœ… Fusion facile des adaptateurs avec le modÃ¨le de base

---

## ğŸ§  Architecture et Algorithmes

### 1. Le ModÃ¨le PrÃ©-entraÃ®nÃ© : Depth Anything V2

**Depth Anything V2** est un modÃ¨le de fondation de pointe pour l'estimation de profondeur monoculaire. Il s'appuie sur une architecture **DPT (Dense Prediction Transformer)** propulsÃ©e par un encodeur **Vision Transformer (ViT)**. Cette architecture permet de capturer des relations globales dans l'image grÃ¢ce au mÃ©canisme d'attention, surpassant les CNNs classiques sur la prÃ©servation des dÃ©tails fins.

**ImplÃ©mentation via Hugging Face :**
Pour ce projet, nous n'avons pas tÃ©lÃ©chargÃ© manuellement les poids depuis le dÃ©pÃ´t GitHub officiel. Nous avons privilÃ©giÃ© l'intÃ©gration native via la bibliothÃ¨que **Transformers** de Hugging Face.

Le modÃ¨le est chargÃ© dynamiquement depuis le **Hugging Face Hub** (ID : `depth-anything/Depth-Anything-V2-Small-hf`). Cette approche simplifie le pipeline (via `AutoModelForDepthEstimation`), assure la compatibilitÃ© des versions et Ã©vite la gestion complexe de fichiers de poids locaux.

#### Architecture du ModÃ¨le

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Depth Anything Small                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Image (H Ã— W Ã— 3)                                    â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Vision Transformer (ViT) Backbone       â”‚        â”‚
â”‚  â”‚  - Patch Embedding (16 Ã— 16 patches)            â”‚        â”‚
â”‚  â”‚  - Multi-Head Self-Attention (Query, Key, Value)â”‚        â”‚
â”‚  â”‚  - Feed-Forward Networks                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚              DPT Decoder Head                   â”‚        â”‚
â”‚  â”‚  - Feature Reassembly                           â”‚        â”‚
â”‚  â”‚  - Progressive Upsampling                       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Output Depth Map (H Ã— W Ã— 1)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CaractÃ©ristiques ClÃ©s

- **ModÃ¨le utilisÃ©** : `LiheYoung/depth-anything-small-hf`
- **ParamÃ¨tres totaux** : ~25.2 millions
- **RÃ©solution d'entrÃ©e** : 518 Ã— 840 pixels (adaptÃ©e aux images Zivid)
- **Sortie** : Carte de profondeur normalisÃ©e [0, 1]

### 2. L'Algorithme de Fine-Tuning : LoRA (Low-Rank Adaptation)

**LoRA** est une technique de fine-tuning efficace qui permet d'adapter de grands modÃ¨les prÃ©-entraÃ®nÃ©s sans modifier leurs poids originaux.

#### Principe MathÃ©matique

Au lieu de mettre Ã  jour les poids $W$ directement, LoRA dÃ©compose la mise Ã  jour en deux matrices de faible rang :

$$
W_{new} = W_{original} + \Delta W = W_{original} + B \cdot A
$$

OÃ¹ :

- $W_{original} \in \mathbb{R}^{d \times k}$ : Poids gelÃ©s du modÃ¨le original
- $A \in \mathbb{R}^{r \times k}$ : Matrice "down-projection" (compresse)
- $B \in \mathbb{R}^{d \times r}$ : Matrice "up-projection" (dÃ©compresse)
- $r$ : Rang (hyperparamÃ¨tre, $r \ll \min(d, k)$)

#### Configuration LoRA UtilisÃ©e

```python
lora_config = LoraConfig(
    r=16,                                    # Rang de la dÃ©composition
    lora_alpha=32,                           # Facteur d'Ã©chelle (Î±/r)
    target_modules=["query", "key", "value"],# Couches d'attention ciblÃ©es
    lora_dropout=0.05,                       # RÃ©gularisation
    bias="none",                             # Pas d'adaptation des biais
)
```

#### Statistiques d'EntraÃ®nement

```
trainable params: 442,368 || all params: 25,227,457 || trainable%: 1.7535
```

---
## ğŸ”§ PrÃ©traitement 
## ğŸ’» DÃ©tails de l'ImplÃ©mentation et Pipeline (Code)

### A. Pipeline de DonnÃ©es Custom (Dataset Zivid)

Le jeu de donnÃ©es Zivid est un dataset industriel contenant des paires image RGB / profondeur XYZ capturÃ©es par une camÃ©ra Zivid.

#### Structure des Fichiers

```
DATASET_DEVOIR/
â”œâ”€â”€ images/                    # Images RGB (.png)
â”‚   â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_color.png
â”‚   â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_color.png
â”‚   â””â”€â”€ ...
â””â”€â”€ depth/                     # Cartes de profondeur XYZ (.npy)
    â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_rawDepth.npy
    â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_rawDepth.npy
    â””â”€â”€ ...
```

#### Statistiques du Dataset

| MÃ©trique                    | Valeur              |
| ---------------------------- | ------------------- |
| Nombre total d'Ã©chantillons | 58                  |
| RÃ©solution des images       | 1200 Ã— 1944 pixels |
| Profondeur MIN               | 251.74 mm           |
| Profondeur MAX               | 3907.45 mm          |
| Moyenne des profondeurs      | 1542.16 mm          |
| Ã‰cart-type                  | 295.35 mm           |
| Pixels valides (moyenne)     | 68.5%               |


### B. Configuration du ModÃ¨le et LoRA

```python
from transformers import AutoModelForDepthEstimation, AutoImageProcessor
from peft import get_peft_model, LoraConfig

# 1. Chargement du modÃ¨le prÃ©-entraÃ®nÃ©
MODEL_NAME = "LiheYoung/depth-anything-small-hf"
base_model = AutoModelForDepthEstimation.from_pretrained(MODEL_NAME)
image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)

# 2. Configuration LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query", "key", "value"],
    lora_dropout=0.05,
    bias="none",
)

# 3. Application de LoRA
model_lora = get_peft_model(base_model, lora_config)
```
# 4) Masque de validitÃ© (NaN / trous capteur)
On construit un masque de pixels valides :
- `Z` fini (pas NaN/inf)
- `0 < Z < 10000` (filtrage valeurs aberrantes)
Les pixels invalides sont remplacÃ©s par 0 pour stocker, mais **ignorÃ©s dans la loss**.

# 5) Normalisation inverse (amÃ©liorer les objets proches)
Au lieu de normaliser linÃ©airement, on applique une normalisation inverse

âœ… Effet : les petites distances (objets proches) occupent une plage plus large â†’ meilleurs dÃ©tails.

# 6) Haute rÃ©solution en entrÃ©e
Dans le `Dataset`, le processor impose :
- **height = 756**
- **width = 1260**
(choisi car multiple de 14, et bon compromis dÃ©tails / mÃ©moire)

---

## ğŸ§¾ EntraÃ®nement 

### 1) Alignement des dimensions
La sortie `predicted_depth` nâ€™a pas forcÃ©ment la taille de la GT.
On **upsample** la prÃ©diction vers la taille GT (1200Ã—1944) via :

- `F.interpolate(..., mode="bicubic", align_corners=False)`

### 2) Loss : L1 masquÃ©e + loss de gradient (bords)

#### a) L1 masquÃ©e
CalculÃ©e uniquement sur les pixels valides :
\[
\mathcal{L}_{L1} = \frac{\sum M| \hat{d}-d |}{\sum M + \varepsilon}
\]

#### b) Loss de gradient (nettetÃ© des contours)
On calcule des gradients par diffÃ©rences finies (x/y) et on applique :
- un masque de validitÃ© voisinage (`mask_x`, `mask_y`)
- une pondÃ©ration plus forte sur les pixels â€œbordsâ€ :
  - seuil `tau = 0.02`
  - multiplicateur `+10` quand `|grad(GT)| > tau`

Loss totale (version finale) :
\[
\mathcal{L} = \mathcal{L}_{L1} + 3.0 \cdot \mathcal{L}_{grad}
\]

### 3) HyperparamÃ¨tres (TrainingArguments)
Configuration finale :
- `num_train_epochs = 15`
- `per_device_train_batch_size = 1` (obligatoire en haute rÃ©solution)
- `gradient_accumulation_steps = 8` (batch effectif â‰ˆ 8)
- `learning_rate = 5e-5`
- `fp16 = True`
- `eval_strategy = "epoch"`
- `save_strategy = "epoch"`
- `load_best_model_at_end = True`
- `output_dir = "./resultats_pneu_v5"` (ou Ã©quivalent)

---

## ğŸš€ Reproduire le projet

### 1) Installation
Option conda :
```bash
conda create -n depth_lora python=3.10 -y
conda activate depth_lora
pip install -r requirements.txt
2) PrÃ©parer le dataset

Place DATASET_DEVOIR/images et DATASET_DEVOIR/depth comme dÃ©crit plus haut.

3) Lancer le notebook

Ouvre le notebook principal (ex. transfomers_code.ipynb) et exÃ©cute les cellules dans lâ€™ordre :

imports / install

lecture dataset + stats globales min/max

crÃ©ation Dataset (use_inverse=True)

chargement modÃ¨le + LoRA

Trainer custom (loss L1 + gradient)

entraÃ®nement + visualisation qualitative
ğŸ§ª InfÃ©rence et dÃ©normalisation (retour en mm)

AprÃ¨s prÃ©diction, si ta sortie est une profondeur normalisÃ©e inverse depth_norm dans [0,1] :
import torch

DEPTH_MIN = 251.74
DEPTH_MAX = 3907.45

depth_min_inv = 1.0 / DEPTH_MAX
depth_max_inv = 1.0 / DEPTH_MIN

# depth_norm : (H,W) en [0,1]
depth_inv = depth_norm * (depth_max_inv - depth_min_inv) + depth_min_inv
depth_mm = 1.0 / (depth_inv + 1e-6)
âš ï¸ Si tu compares Ã  la GT en mm : applique le masque (pixels valides uniquement).
ğŸ§© Arborescence
Projet_Transformers/
â”œâ”€â”€ transfomers_code.ipynb              # Notebook final
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ DATASET_DEVOIR/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ depth/
â””â”€â”€ resultats_pneu_v5/
    â”œâ”€â”€ checkpoint-.../
    â””â”€â”€ ...
ğŸ› ï¸ DÃ©pannage rapide

CUDA OOM (mÃ©moire GPU) :

garder batch_size=1

augmenter gradient_accumulation_steps

rÃ©duire la rÃ©solution (si nÃ©cessaire)

Profondeur â€œpas parfaiteâ€ sur pneus :

GT bruitÃ©e/incomplÃ¨te

pneus sombres/reflets â†’ ambiguÃ¯tÃ©s monoculaires

upsampling bicubique aide, mais les micro-dÃ©tails restent difficiles
ğŸ“š RÃ©fÃ©rences

Depth Anything (arXiv): https://arxiv.org/abs/2401.10891

LoRA (arXiv): https://arxiv.org/abs/2106.09685

Transformers docs: https://huggingface.co/docs/transformers

PEFT docs: https://huggingface.co/docs/peft
