# ğŸ¯ Projet Devoir : Fine-Tuning de Depth Anything V2 avec LoRA (Transformers)

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.30+-FFD21E?style=for-the-badge)
![PEFT](https://img.shields.io/badge/PEFT-LoRA-6A5ACD?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

</div>

---

## ğŸ‘¥ Auteurs

| Nom |
| --- |
| **Abdelali Chikhi** |
| **Ayman Zejli** |
| **Mouad Azennag** |
| **Loic Magnan** |

---

## ğŸ“– Contexte et Objectif

Lâ€™estimation de profondeur monoculaire (MDE) consiste Ã  prÃ©dire une carte de profondeur dense Ã  partir dâ€™une seule image RGB.

**Objectif du projet :** adapter **Depth Anything V2** (Transformers) au dataset industriel **Zivid** (paires RGB + nuage de points XYZ par pixel) en utilisant **LoRA** (fine-tuning paramÃ¨tre-efficiente).

ğŸ¯ Focus de la version finale : amÃ©liorer la prÃ©cision sur les **objets proches** et les **dÃ©tails fins** (ex. contours / rainures de pneus) grÃ¢ce Ã  :
- une **normalisation inverse** de la profondeur,
- une **loss mixte** : **L1 masquÃ©e + loss de gradient** (bords),
- une entrÃ©e **haute rÃ©solution** et un **upsampling bicubique** vers la GT.

---

## ğŸ§  ModÃ¨le & MÃ©thode

### 1) ModÃ¨le prÃ©-entraÃ®nÃ© : Depth Anything V2 (HF)

- **Model ID (Hugging Face)** : `depth-anything/Depth-Anything-V2-Small-hf`
- Chargement via Transformers :
  - `AutoImageProcessor`
  - `AutoModelForDepthEstimation`

### 2) Fine-tuning LoRA (PEFT)

LoRA apprend une mise Ã  jour de rang faible :

\[
W' = W + \Delta W,\quad \Delta W = BA
\]

Configuration LoRA (version finale) :
- `r = 16`
- `lora_alpha = 32`
- `target_modules = ["query","key","value"]`
- `lora_dropout = 0.05`
- `bias = "none"`

---

## ğŸ“‚ Dataset Zivid & Structure attendue

Chaque Ã©chantillon :
- une image RGB (`.png`)
- un fichier profondeur (`.npy`) de shape `(H, W, 3)` contenant `(X, Y, Z)` par pixel
- la GT profondeur = **canal Z** en **mm**

Structure recommandÃ©e :
DATASET_DEVOIR/
â”œâ”€â”€ images/ # Images RGB (.png)
â””â”€â”€ depth/ # Nuages de points XYZ (.npy)

### Statistiques typiques (dataset fourni)
- Nb total : **58** Ã©chantillons
- RÃ©solution brute : **1200 Ã— 1944**
- Profondeur min/max (mm) : **251.74** / **3907.45**

---

## ğŸ”§ PrÃ©traitement (version finale)

### 1) Masque de validitÃ© (NaN / trous capteur)
On construit un masque de pixels valides :
- `Z` fini (pas NaN/inf)
- `0 < Z < 10000` (filtrage valeurs aberrantes)
Les pixels invalides sont remplacÃ©s par 0 pour stocker, mais **ignorÃ©s dans la loss**.

### 2) Normalisation inverse (amÃ©liorer les objets proches)
Au lieu de normaliser linÃ©airement, on applique :

\[
z_{inv} = \frac{1}{z + \varepsilon}
\]

Avec :
\[
z_{min}^{inv} = \frac{1}{z_{max}},\quad z_{max}^{inv} = \frac{1}{z_{min}}
\]
\[
z_{norm} = \mathrm{clip}\left(\frac{z_{inv}-z_{min}^{inv}}{z_{max}^{inv}-z_{min}^{inv}}, 0, 1\right)
\]

âœ… Effet : les petites distances (objets proches) occupent une plage plus large â†’ meilleurs dÃ©tails.

### 3) Haute rÃ©solution en entrÃ©e
Dans le `Dataset`, le processor impose :
- **height = 756**
- **width = 1260**
(choisi car multiple de 14, et bon compromis dÃ©tails / mÃ©moire)

---

## ğŸ§¾ EntraÃ®nement (version finale)

### 1) Alignement des dimensions
La sortie `predicted_depth` nâ€™a pas forcÃ©ment la taille de la GT.
On **upsample** la prÃ©diction vers la taille GT (1200Ã—1944) via :

- `F.interpolate(..., mode="bicubic", align_corners=False)`

### 2) Loss : L1 masquÃ©e + loss de gradient (bords)

#### a) L1 masquÃ©e
CalculÃ©e uniquement sur les pixels valides :
\[
\mathcal{L}_{L1} = \frac{\sum M| \hat{d}-d |}{\sum M + \varepsilon}
\]

#### b) Loss de gradient (nettetÃ© des contours)
On calcule des gradients par diffÃ©rences finies (x/y) et on applique :
- un masque de validitÃ© voisinage (`mask_x`, `mask_y`)
- une pondÃ©ration plus forte sur les pixels â€œbordsâ€ :
  - seuil `tau = 0.02`
  - multiplicateur `+10` quand `|grad(GT)| > tau`

Loss totale (version finale) :
\[
\mathcal{L} = \mathcal{L}_{L1} + 3.0 \cdot \mathcal{L}_{grad}
\]

### 3) HyperparamÃ¨tres (TrainingArguments)
Configuration finale :
- `num_train_epochs = 15`
- `per_device_train_batch_size = 1` (obligatoire en haute rÃ©solution)
- `gradient_accumulation_steps = 8` (batch effectif â‰ˆ 8)
- `learning_rate = 5e-5`
- `fp16 = True`
- `eval_strategy = "epoch"`
- `save_strategy = "epoch"`
- `load_best_model_at_end = True`
- `output_dir = "./resultats_pneu_v5"` (ou Ã©quivalent)

---

## ğŸš€ Reproduire le projet

### 1) Installation
Option conda :
```bash
conda create -n depth_lora python=3.10 -y
conda activate depth_lora
pip install -r requirements.txt
2) PrÃ©parer le dataset

Place DATASET_DEVOIR/images et DATASET_DEVOIR/depth comme dÃ©crit plus haut.

3) Lancer le notebook

Ouvre le notebook principal (ex. transfomers_code.ipynb) et exÃ©cute les cellules dans lâ€™ordre :

imports / install

lecture dataset + stats globales min/max

crÃ©ation Dataset (use_inverse=True)

chargement modÃ¨le + LoRA

Trainer custom (loss L1 + gradient)

entraÃ®nement + visualisation qualitative
ğŸ§ª InfÃ©rence et dÃ©normalisation (retour en mm)

AprÃ¨s prÃ©diction, si ta sortie est une profondeur normalisÃ©e inverse depth_norm dans [0,1] :
import torch

DEPTH_MIN = 251.74
DEPTH_MAX = 3907.45

depth_min_inv = 1.0 / DEPTH_MAX
depth_max_inv = 1.0 / DEPTH_MIN

# depth_norm : (H,W) en [0,1]
depth_inv = depth_norm * (depth_max_inv - depth_min_inv) + depth_min_inv
depth_mm = 1.0 / (depth_inv + 1e-6)
âš ï¸ Si tu compares Ã  la GT en mm : applique le masque (pixels valides uniquement).
ğŸ§© Arborescence
Projet_Transformers/
â”œâ”€â”€ transfomers_code.ipynb              # Notebook final
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ DATASET_DEVOIR/
â”‚   â”œâ”€â”€ images/
â”‚   â””â”€â”€ depth/
â””â”€â”€ resultats_pneu_v5/
    â”œâ”€â”€ checkpoint-.../
    â””â”€â”€ ...
ğŸ› ï¸ DÃ©pannage rapide

CUDA OOM (mÃ©moire GPU) :

garder batch_size=1

augmenter gradient_accumulation_steps

rÃ©duire la rÃ©solution (si nÃ©cessaire)

Profondeur â€œpas parfaiteâ€ sur pneus :

GT bruitÃ©e/incomplÃ¨te

pneus sombres/reflets â†’ ambiguÃ¯tÃ©s monoculaires

upsampling bicubique aide, mais les micro-dÃ©tails restent difficiles
ğŸ“š RÃ©fÃ©rences

Depth Anything (arXiv): https://arxiv.org/abs/2401.10891

LoRA (arXiv): https://arxiv.org/abs/2106.09685

Transformers docs: https://huggingface.co/docs/transformers

PEFT docs: https://huggingface.co/docs/peft
