# ğŸ¯ Projet Devoir : Fine-Tuning de Depth Anything avec LoRA

<div align="center">

![Python](https://img.shields.io/badge/Python-3.10+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-EE4C2C?style=for-the-badge&logo=pytorch&logoColor=white)
![HuggingFace](https://img.shields.io/badge/ğŸ¤—_Transformers-4.30+-FFD21E?style=for-the-badge)
![License](https://img.shields.io/badge/License-MIT-green?style=for-the-badge)

</div>

---

## ğŸ‘¥ Auteurs

| Nom                  |
| -------------------- | 
| **Abdelali Chikhi** | 
| **Ayman Zejli**      | 
| **Mouad Azenag**      | 
| **Loic Magnan**      | 

---

## ğŸ“– Contexte et Objectif

### Contexte

L'estimation de profondeur monoculaire est une tÃ¢che fondamentale en vision par ordinateur qui consiste Ã  prÃ©dire la distance des objets dans une scÃ¨ne Ã  partir d'une seule image RGB. Cette capacitÃ© est cruciale pour de nombreuses applications :

- ğŸš— **VÃ©hicules autonomes** : Navigation et Ã©vitement d'obstacles
- ğŸ¤– **Robotique** : Manipulation d'objets et navigation
- ğŸ­ **Industrie 4.0** : ContrÃ´le qualitÃ© et inspection automatisÃ©e
- ğŸ® **RÃ©alitÃ© augmentÃ©e** : Placement prÃ©cis d'objets virtuels

### Objectif du Projet

Ce projet vise Ã  **adapter le modÃ¨le Depth Anything** (un modÃ¨le prÃ©-entraÃ®nÃ© de pointe pour l'estimation de profondeur) au **jeu de donnÃ©es Zivid** spÃ©cifique Ã  un contexte industriel, en utilisant la technique de **LoRA (Low-Rank Adaptation)** pour un fine-tuning efficace.

#### Pourquoi LoRA ?

- âœ… RÃ©duction drastique des paramÃ¨tres entraÃ®nables (~1.75% des paramÃ¨tres totaux)
- âœ… PrÃ©servation des connaissances du modÃ¨le prÃ©-entraÃ®nÃ©
- âœ… EntraÃ®nement rapide avec moins de ressources GPU
- âœ… Fusion facile des adaptateurs avec le modÃ¨le de base

---

## ğŸ§  Architecture et Algorithmes

### 1. Le ModÃ¨le PrÃ©-entraÃ®nÃ© : Depth Anything V2

**Depth Anything V2** est un modÃ¨le de fondation de pointe pour l'estimation de profondeur monoculaire. Il s'appuie sur une architecture **DPT (Dense Prediction Transformer)** propulsÃ©e par un encodeur **Vision Transformer (ViT)**. Cette architecture permet de capturer des relations globales dans l'image grÃ¢ce au mÃ©canisme d'attention, surpassant les CNNs classiques sur la prÃ©servation des dÃ©tails fins.

**ImplÃ©mentation via Hugging Face :**
Pour ce projet, nous n'avons pas tÃ©lÃ©chargÃ© manuellement les poids depuis le dÃ©pÃ´t GitHub officiel. Nous avons privilÃ©giÃ© l'intÃ©gration native via la bibliothÃ¨que **Transformers** de Hugging Face.

Le modÃ¨le est chargÃ© dynamiquement depuis le **Hugging Face Hub** (ID : `depth-anything/Depth-Anything-V2-Small-hf`). Cette approche simplifie le pipeline (via `AutoModelForDepthEstimation`), assure la compatibilitÃ© des versions et Ã©vite la gestion complexe de fichiers de poids locaux.

#### Architecture du ModÃ¨le

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Depth Anything Small                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Input Image (H Ã— W Ã— 3)                                    â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚         Vision Transformer (ViT) Backbone       â”‚        â”‚
â”‚  â”‚  - Patch Embedding (16 Ã— 16 patches)            â”‚        â”‚
â”‚  â”‚  - Multi-Head Self-Attention (Query, Key, Value)â”‚        â”‚
â”‚  â”‚  - Feed-Forward Networks                        â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚              DPT Decoder Head                   â”‚        â”‚
â”‚  â”‚  - Feature Reassembly                           â”‚        â”‚
â”‚  â”‚  - Progressive Upsampling                       â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚         â”‚                                                    â”‚
â”‚         â–¼                                                    â”‚
â”‚  Output Depth Map (H Ã— W Ã— 1)                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### CaractÃ©ristiques ClÃ©s

- **ModÃ¨le utilisÃ©** : `LiheYoung/depth-anything-small-hf`
- **ParamÃ¨tres totaux** : ~25.2 millions
- **RÃ©solution d'entrÃ©e** : 518 Ã— 840 pixels (adaptÃ©e aux images Zivid)
- **Sortie** : Carte de profondeur normalisÃ©e [0, 1]

### 2. L'Algorithme de Fine-Tuning : LoRA (Low-Rank Adaptation)

**LoRA** est une technique de fine-tuning efficace qui permet d'adapter de grands modÃ¨les prÃ©-entraÃ®nÃ©s sans modifier leurs poids originaux.

#### Principe MathÃ©matique

Au lieu de mettre Ã  jour les poids $W$ directement, LoRA dÃ©compose la mise Ã  jour en deux matrices de faible rang :

$$
W_{new} = W_{original} + \Delta W = W_{original} + B \cdot A
$$

OÃ¹ :

- $W_{original} \in \mathbb{R}^{d \times k}$ : Poids gelÃ©s du modÃ¨le original
- $A \in \mathbb{R}^{r \times k}$ : Matrice "down-projection" (compresse)
- $B \in \mathbb{R}^{d \times r}$ : Matrice "up-projection" (dÃ©compresse)
- $r$ : Rang (hyperparamÃ¨tre, $r \ll \min(d, k)$)

#### Configuration LoRA UtilisÃ©e

```python
lora_config = LoraConfig(
    r=16,                                    # Rang de la dÃ©composition
    lora_alpha=32,                           # Facteur d'Ã©chelle (Î±/r)
    target_modules=["query", "key", "value"],# Couches d'attention ciblÃ©es
    lora_dropout=0.05,                       # RÃ©gularisation
    bias="none",                             # Pas d'adaptation des biais
)
```

#### Statistiques d'EntraÃ®nement

```
trainable params: 442,368 || all params: 25,227,457 || trainable%: 1.7535
```

---

## ğŸ’» DÃ©tails de l'ImplÃ©mentation et Pipeline (Code)

### A. Pipeline de DonnÃ©es Custom (Dataset Zivid)

Le jeu de donnÃ©es Zivid est un dataset industriel contenant des paires image RGB / profondeur XYZ capturÃ©es par une camÃ©ra Zivid.

#### Structure des Fichiers

```
DATASET_DEVOIR/
â”œâ”€â”€ images/                    # Images RGB (.png)
â”‚   â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_color.png
â”‚   â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_color.png
â”‚   â””â”€â”€ ...
â””â”€â”€ depth/                     # Cartes de profondeur XYZ (.npy)
    â”œâ”€â”€ 21-12-03-18-52-37_Zivid_acquisition_rawDepth.npy
    â”œâ”€â”€ 22-09-21-14-12-11_Zivid_acquisition_rawDepth.npy
    â””â”€â”€ ...
```

#### Statistiques du Dataset

| MÃ©trique                    | Valeur              |
| ---------------------------- | ------------------- |
| Nombre total d'Ã©chantillons | 58                  |
| RÃ©solution des images       | 1200 Ã— 1944 pixels |
| Profondeur MIN               | 251.74 mm           |
| Profondeur MAX               | 3907.45 mm          |
| Moyenne des profondeurs      | 1542.16 mm          |
| Ã‰cart-type                  | 295.35 mm           |
| Pixels valides (moyenne)     | 68.5%               |


### B. Configuration du ModÃ¨le et LoRA

```python
from transformers import AutoModelForDepthEstimation, AutoImageProcessor
from peft import get_peft_model, LoraConfig

# 1. Chargement du modÃ¨le prÃ©-entraÃ®nÃ©
MODEL_NAME = "LiheYoung/depth-anything-small-hf"
base_model = AutoModelForDepthEstimation.from_pretrained(MODEL_NAME)
image_processor = AutoImageProcessor.from_pretrained(MODEL_NAME)

# 2. Configuration LoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["query", "key", "value"],
    lora_dropout=0.05,
    bias="none",
)

# 3. Application de LoRA
model_lora = get_peft_model(base_model, lora_config)
```

### C. Boucle d'EntraÃ®nement (Trainer API)

#### Arguments d'EntraÃ®nement

```python
training_args = TrainingArguments(
    output_dir="./depth_anything_finetuned_lora_zivid",
    learning_rate=5e-5,
    num_train_epochs=10,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    eval_steps=500,
    save_steps=500,
    logging_steps=50,
    report_to="tensorboard",
    save_total_limit=3,
    fp16=True,                    # Mixed precision pour accÃ©lÃ©ration
    remove_unused_columns=False,
)
```

#### Trainer PersonnalisÃ© avec Loss MasquÃ©e

```python
class DepthTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        # Forward du modÃ¨le
        outputs = model(pixel_values=inputs.get("pixel_values"))
        predicted_depth = outputs.predicted_depth
      
        # RÃ©cupÃ©ration des labels et masque
        labels = inputs.get("labels")
        valid_mask = inputs.get("valid_mask")
      
        # Interpolation pour aligner les dimensions
        labels = F.interpolate(labels.unsqueeze(1), size=predicted_depth.shape[-2:])
        valid_mask = F.interpolate(valid_mask.unsqueeze(1), size=predicted_depth.shape[-2:])
      
        # MSE masquÃ©e (on ignore les pixels NaN/invalides)
        diff = (predicted_depth - labels) ** 2
        masked_diff = diff * valid_mask
        loss = masked_diff.sum() / (valid_mask.sum() + 1e-8)
      
        return (loss, outputs) if return_outputs else loss
```

### D. InfÃ©rence et Calcul MÃ©trique "RÃ©el"

```python
# Chargement du modÃ¨le fine-tunÃ©
from peft import PeftModel

model = AutoModelForDepthEstimation.from_pretrained(MODEL_NAME)
model = PeftModel.from_pretrained(model, "./depth_anything_finetuned_lora_zivid")

# InfÃ©rence
with torch.no_grad():
    outputs = model(pixel_values=image_tensor)
    predicted_depth = outputs.predicted_depth

# DÃ©-normalisation pour obtenir les valeurs en mm
depth_mm = predicted_depth * (DEPTH_MAX - DEPTH_MIN) + DEPTH_MIN
```

---

## ğŸ“Š Analyse des Performances

### RÃ©sultats du Dernier Run

#### MÃ©triques d'EntraÃ®nement

| MÃ©trique                          | Valeur                                      |
| ---------------------------------- | ------------------------------------------- |
| **Steps totaux**             | 120                                         |
| **Epochs**                   | 10                                          |
| **Temps d'exÃ©cution total** | **37 min 29 sec** (~2273.52 secondes) |
| **Samples par seconde**      | 0.202                                       |
| **Steps par seconde**        | 0.053                                       |
| **FLOPS totaux**             | 9.09 Ã— 10Â¹â¶                              |

#### Ã‰volution de la Loss

| Step | Training Loss    | Learning Rate |
| ---- | ---------------- | ------------- |
| 50   | **1.2008** | 3.0 Ã— 10â»âµ |
| 100  | **0.0441** | 9.0 Ã— 10â»â¶ |

#### Loss Finale

- **Train Loss Finale** : `0.0441`
- **Train Loss Moyenne** : `0.5232`

### InterprÃ©tation des RÃ©sultats

1. **Convergence Rapide** : La loss chute drastiquement de 1.2 Ã  0.044 en seulement 100 steps, dÃ©montrant l'efficacitÃ© de LoRA pour l'adaptation de domaine.
2. **EfficacitÃ© du Fine-Tuning** : Avec seulement 1.75% des paramÃ¨tres entraÃ®nÃ©s, le modÃ¨le atteint une loss trÃ¨s faible sur le dataset Zivid.
3. **Temps d'EntraÃ®nement Raisonnable** : ~38 minutes pour 10 epochs sur un dataset de 58 images haute rÃ©solution.

---

## ğŸ“‚ Livrables

```
Projet_Transfomers/
â”œâ”€â”€ ğŸ““ projet_transfomers.ipynb        # Notebook principal avec tout le code
â”œâ”€â”€ ğŸ“„ README.md                        # Ce fichier de documentation
â”œâ”€â”€ ğŸ“‹ requirements.txt                 # DÃ©pendances Python
â”œâ”€â”€ ğŸ“ DATASET_DEVOIR/                  # Dataset Zivid (58 paires image/depth)
â”‚   â”œâ”€â”€ images/                         # Images RGB (.png)
â”‚   â””â”€â”€ depth/                          # Cartes de profondeur XYZ (.npy)
â”œâ”€â”€ ğŸ“ depth_anything_finetuned_lora_zivid/  # ModÃ¨le fine-tunÃ©
â”‚   â”œâ”€â”€ checkpoint-120/                 # Dernier checkpoint
â”‚   â””â”€â”€ runs/                           # Logs TensorBoard
â””â”€â”€ ğŸ“„ Sujet FineTuning Transformers.pdf # Ã‰noncÃ© du devoir
```

---

## ğŸš€ Guide de Reproduction

### 1. Installation de l'Environnement

#### Option A : Installation avec Conda 

```bash
# CrÃ©er un environnement conda
conda create -n transformers_depth python=3.11 -y
conda activate transformers_depth

# Installer les dÃ©pendances
pip install -r requirements.txt
```

#### VÃ©rifier l'installation CUDA

```python
import torch
print(f"CUDA disponible: {torch.cuda.is_available()}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
```

### 3. PrÃ©paration des DonnÃ©es

Le dataset doit suivre la structure suivante :

```
DATASET_DEVOIR/
â”œâ”€â”€ images/
â”‚   â””â”€â”€ *.png        # Images RGB
â””â”€â”€ depth/
    â””â”€â”€ *.npy        # Fichiers XYZ (profondeur en mm)
```

âš ï¸ **Important** : Les fichiers `.npy` doivent avoir la shape `(H, W, 3)` oÃ¹ le canal 2 (Z) contient les valeurs de profondeur en millimÃ¨tres.

### 4. Lancement du Fine-Tuning

1. **Ouvrir le notebook** :

```bash
jupyter notebook projet_transfomers.ipynb
```

2. **ExÃ©cuter les cellules** dans l'ordre :

   - ğŸ“¦ Installation des dÃ©pendances
   - ğŸ“Š Analyse du dataset Zivid
   - ğŸ”§ Configuration du modÃ¨le et LoRA
   - ğŸš€ EntraÃ®nement
   - ğŸ“ˆ Visualisation des rÃ©sultats
3. **Suivre l'entraÃ®nement avec TensorBoard** :

```bash
tensorboard --logdir ./depth_anything_finetuned_lora_zivid
```

### 5. InfÃ©rence avec le ModÃ¨le Fine-TunÃ©

```python
from transformers import AutoModelForDepthEstimation, AutoImageProcessor
from peft import PeftModel
from PIL import Image
import torch

# Charger le modÃ¨le
MODEL_NAME = "LiheYoung/depth-anything-small-hf"
model = AutoModelForDepthEstimation.from_pretrained(MODEL_NAME)
model = PeftModel.from_pretrained(model, "./depth_anything_finetuned_lora_zivid")
processor = AutoImageProcessor.from_pretrained(MODEL_NAME)

# Charger une image
image = Image.open("votre_image.png")
inputs = processor(images=image, return_tensors="pt")

# InfÃ©rence
model.eval()
with torch.no_grad():
    outputs = model(**inputs)
    depth = outputs.predicted_depth

# Visualiser
import matplotlib.pyplot as plt
plt.imshow(depth.squeeze().numpy(), cmap='plasma')
plt.colorbar(label='Profondeur normalisÃ©e')
plt.show()
```

---

## ğŸ“š RÃ©fÃ©rences

- [Depth Anything Paper](https://arxiv.org/abs/2401.10891)
- [LoRA: Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2106.09685)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [PEFT Library](https://huggingface.co/docs/peft)

---

## ğŸ“ License

Ce projet est rÃ©alisÃ© dans le cadre d'un devoir acadÃ©mique. Tous droits rÃ©servÃ©s aux auteurs.

---

<div align="center">
  <sub>Projet Transformers - Polytech Clermont IMDS5A</sub>
</div>
